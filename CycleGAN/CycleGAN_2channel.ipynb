{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "anSn07EhuGES",
    "tags": []
   },
   "source": [
    "# Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 2758,
     "status": "ok",
     "timestamp": 1655719006528,
     "user": {
      "displayName": "Alice Henriques da Rosa",
      "userId": "12243278505695630565"
     },
     "user_tz": -60
    },
    "id": "Kb0w8OA2tzMj"
   },
   "outputs": [],
   "source": [
    "# example of training a cyclegan on the horse2zebra dataset\n",
    "from random import random\n",
    "import numpy as np\n",
    "from numpy import load\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy import asarray\n",
    "from numpy.random import randint\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.initializers import RandomNorm \n",
    "from keras.models import Model\n",
    "from keras.models import Input\n",
    "from keras.layers import Conv3D, BatchNormalization\n",
    "from keras.layers import Conv3DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import UpSampling3D\n",
    "from keras.activations import sigmoid\n",
    "from matplotlib import pyplot\n",
    "import os\n",
    "from tifffile import imread, imsave, imwrite\n",
    "from sys import stdout\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "from keras import backend as K\n",
    "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow_addons.layers import SpectralNormalization\n",
    "import xlwt\n",
    "from xlwt import Workbook\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.nn import ReflectionPad3d\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_TQLQxzuJIz",
    "tags": []
   },
   "source": [
    "# Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "SW_t-_lNt2jy"
   },
   "outputs": [],
   "source": [
    "# define the discriminator model\n",
    "def define_discriminator(image_shape):\n",
    "    # weight initialization\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    # source image input\n",
    "    in_image = Input(shape=image_shape)\n",
    "\n",
    "    # C64\n",
    "    d = Conv3D(64, 4, strides=2, padding='same', kernel_initializer=init)(in_image)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # C128\n",
    "    d = Conv3D(128, 4, strides=2, padding='same', kernel_initializer=init)(d)\n",
    "    d = InstanceNormalization(axis=-1)(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # C256\n",
    "    d = Conv3D(256, 4, strides=2, padding='same', kernel_initializer=init)(d)\n",
    "    d = InstanceNormalization(axis=-1)(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # C512\n",
    "    d = Conv3D(512, 4, strides=2, padding='same', kernel_initializer=init)(d)\n",
    "    d = InstanceNormalization(axis=-1)(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # second last output layer\n",
    "    d = Conv3D(512, 4, padding='same', kernel_initializer=init)(d)\n",
    "    d = InstanceNormalization(axis=-1)(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # patch output\n",
    "    patch_out = Conv3D(1, 4, padding='same', kernel_initializer=init)(d)\n",
    "    # define model\n",
    "    model = Model(in_image, patch_out)\n",
    "    # compile model\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=0.0001, beta_1=0.5), loss_weights=[0.5])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9ylsvemot8OV"
   },
   "outputs": [],
   "source": [
    "# generator a resnet block\n",
    "def resnet_block(n_filters, input_layer):\n",
    "    # weight initialization\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    # first layer convolutional layer\n",
    "    g = Conv3D(n_filters, 3, padding='same', kernel_initializer=init)(input_layer)\n",
    "    g = InstanceNormalization(axis=-1)(g)\n",
    "    g = Activation('relu')(g)\n",
    "    # second convolutional layer\n",
    "    g = Conv3D(n_filters, 3, padding='same', kernel_initializer=init)(g)\n",
    "    g = InstanceNormalization(axis=-1)(g)\n",
    "    # concatenate merge channel-wise with input layer\n",
    "    g = Concatenate()([g, input_layer])\n",
    "    return g\n",
    "\n",
    "# define the standalone generator model\n",
    "def define_generator(image_shape, n_resnet=9):\n",
    "\n",
    "\t# weight initialization\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    # image input\n",
    "    in_image = Input(shape=image_shape)\n",
    "    # c7s1-32\n",
    "    g = Conv3D(32, 7, padding='same', kernel_initializer=init)(in_image)\n",
    "    g = InstanceNormalization(axis=-1)(g)\n",
    "    g = Activation('relu')(g)\n",
    "    # d64\n",
    "    g = Conv3D(64, 3, strides=2, padding='same', kernel_initializer=init)(g)\n",
    "    g = InstanceNormalization(axis=-1)(g)\n",
    "    g = Activation('relu')(g)\n",
    "    # d128\n",
    "    g = Conv3D(128, 3, strides=2, padding='same', kernel_initializer=init)(g)\n",
    "    g = InstanceNormalization(axis=-1)(g)\n",
    "    g = Activation('relu')(g)\n",
    "    # R256\n",
    "    for _ in range(n_resnet):\n",
    "        g = resnet_block(128, g)\n",
    "    # u64\n",
    "    g = Conv3DTranspose(64, 3, strides=2, padding='same', kernel_initializer=init)(g)\n",
    "    g = InstanceNormalization(axis=-1)(g)\n",
    "    g = Activation('relu')(g)\n",
    "    # u32\n",
    "    g = Conv3DTranspose(32, 3, strides=2, padding='same', kernel_initializer=init)(g)\n",
    "    g = InstanceNormalization(axis=-1)(g)\n",
    "    g = Activation('relu')(g)\n",
    "    # c7s1-2\n",
    "    g = Conv3D(2, 7, padding='same', kernel_initializer=init)(g)\n",
    "    g = InstanceNormalization(axis=-1)(g)\n",
    "    out_image = Activation('tanh')(g)\n",
    "    # define model\n",
    "    model = Model(in_image, out_image)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6oZZS6Gbt_hX"
   },
   "outputs": [],
   "source": [
    "def weighted_mean_absolute_error(class_weights):\n",
    "    def loss(y_true, y_pred):\n",
    "        mae = tf.keras.losses.MeanAbsoluteError()\n",
    "        red_channel_loss = mae(y_true[:,:,:,:,0],y_pred[:,:,:,:,0])\n",
    "        green_channel_loss = mae(y_true[:,:,:,:,1],y_pred[:,:,:,:,1])\n",
    "        return class_weights[0]*red_channel_loss+class_weights[1]*green_channel_loss\n",
    "    return loss\n",
    "\n",
    "# define a composite model for updating generators by adversarial and cycle loss\n",
    "def define_composite_model(g_model_1, d_model, g_model_2, image_shape, class_weights=[3.5,1]):\n",
    "    # ensure the model we're updating is trainable\n",
    "    g_model_1.trainable = True\n",
    "    # mark discriminator as not trainable\n",
    "    d_model.trainable = False\n",
    "    # mark other generator model as not trainable\n",
    "    g_model_2.trainable = False\n",
    "    # discriminator element\n",
    "    input_gen = Input(shape=image_shape)\n",
    "    gen1_out = g_model_1(input_gen)\n",
    "    output_d = d_model(gen1_out)\n",
    "    # identity element\n",
    "    input_id = Input(shape=image_shape)\n",
    "    output_id = g_model_1(input_id)\n",
    "    # forward cycle\n",
    "    output_f = g_model_2(gen1_out)\n",
    "    # backward cycle\n",
    "    gen2_out = g_model_2(input_id)\n",
    "    output_b = g_model_1(gen2_out)\n",
    "    # define model graph\n",
    "    model = Model([input_gen, input_id], [output_d, output_id, output_f, output_b])\n",
    "    # define optimization algorithm configuration\n",
    "    opt = Adam(learning_rate=0.0005, beta_1=0.5)\n",
    "    # compile model with weighting of least squares loss and L1 loss\n",
    "    wmae = weighted_mean_absolute_error(class_weights)\n",
    "\n",
    "    model.compile(loss=['mse', wmae, wmae, wmae], loss_weights=[1, 5, 10, 10], optimizer=opt)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GcZ9nLp6uMAQ",
    "tags": []
   },
   "source": [
    "# Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "UoX78GBw46io"
   },
   "outputs": [],
   "source": [
    "# load and prepare training images\n",
    "def load_real_samples(dataset, ID, filename, dim):\n",
    "    # load the dataset\n",
    "    if dataset == '/Images/':\n",
    "        dim = (64,64,64,3)\n",
    "    X = np.empty((len(ID), *dim))\n",
    "\n",
    "    for i, ID_path in enumerate(ID):\n",
    "        X[i,] = imread(filename + dataset + ID_path)\n",
    "        X[i,] = (X[i,] - 0.5) / 0.5\n",
    "\n",
    "    if dataset == '/Images/':\n",
    "        return X[:,:,:,:,:2]\n",
    "    else:\n",
    "        return X\n",
    "\n",
    "\n",
    "# select a batch of random samples, returns images and target\n",
    "def generate_real_samples(dataset, n_samples, patch_size, filename, n_patch):\n",
    "\n",
    "\tlist_IDs = os.listdir(filename + dataset)\n",
    "\t# choose random instances\n",
    "\tindexes = randint(0, len(list_IDs), n_samples)\n",
    "\tlist_IDs_temp = [list_IDs[k] for k in indexes]\n",
    "\t# retrieve selected images\n",
    "\tX = load_real_samples(dataset, list_IDs_temp, filename, patch_size)\n",
    "\t# generate 'real' class labels (1)\n",
    "\ty = ones((n_samples, n_patch, n_patch, 1))\n",
    "\n",
    "\treturn X, y\n",
    "\n",
    "# generate a batch of images, returns images and targets\n",
    "def generate_fake_samples(g_model, dataset, patch_shape):\n",
    "\t# generate fake instance\n",
    "\tX = g_model.predict(dataset)\n",
    "\t# create 'fake' class labels (0)\n",
    "\ty = zeros((len(X), patch_shape, patch_shape, 1))\n",
    "\treturn X, y\n",
    "\n",
    "# save the generator models to file\n",
    "def save_models(step, g_model_AtoB, g_model_BtoA):\n",
    "\t# path to save models\n",
    "\tpath_models = './Models_CycleGAN/'\n",
    "\tif os.path.exists(path_models)==False:\n",
    "\t\tos.mkdir(path_models)\n",
    "\t# save the first generator model\n",
    "\tfilename1 = path_models + 'g_model_AtoB_%03d.h5' % (step+1)\n",
    "\tg_model_AtoB.save(filename1)\n",
    "\t# save the second generator model\n",
    "\tfilename2 = path_models + 'g_model_BtoA_%03d.h5' % (step+1)\n",
    "\tg_model_BtoA.save(filename2)\n",
    "\tprint('>Saved: %s and %s \\n' % (filename1, filename2))\n",
    " \n",
    "\n",
    "def thresholding(patch_prediction):\n",
    "    patch_ind_nuclei = np.argwhere(patch_prediction[:,:,:,0] > 0.5)\n",
    "    patch_ind_golgi = np.argwhere(patch_prediction[:,:,:,1] > 0.5)\n",
    "\n",
    "    patch_prediction_thr = np.zeros((patch_prediction.shape[0],patch_prediction.shape[1],patch_prediction.shape[2],3))\n",
    "\n",
    "    for i in range(patch_ind_nuclei.shape[0]):\n",
    "      patch_prediction_thr[patch_ind_nuclei[i,0],patch_ind_nuclei[i,1],patch_ind_nuclei[i,2],0]=1 \n",
    "\n",
    "    for j in range(patch_ind_golgi.shape[0]):\n",
    "      patch_prediction_thr[patch_ind_golgi[j,0],patch_ind_golgi[j,1],patch_ind_golgi[j,2],1]=1\n",
    "\n",
    "    return patch_prediction_thr\n",
    "\n",
    "def padding(image,size):\n",
    "    \n",
    "    img_reshape = np.moveaxis(image, -1, 0)\n",
    "    \n",
    "    m = ReflectionPad3d((0,size[2]-image.shape[2],0,size[1]-image.shape[1],0,size[0]-image.shape[0]))\n",
    "    _input = torch.tensor(img_reshape, dtype=torch.float)\n",
    "    output = m(_input)\n",
    "    pad_img = output.numpy()\n",
    "    pad_img = np.moveaxis(pad_img, 0, -1)\n",
    "    \n",
    "    return pad_img\n",
    "\n",
    "# generate samples and save as a plot and save the model\n",
    "def summarize_performance(step, g_model, dataset, name, patch_size, results_path, n_samples=5, filename= './Dataset/Patches_64_cycleGAN/Train'):\n",
    "\t# select a sample of input images\n",
    "\tX_in, _ = generate_real_samples(dataset, n_samples, patch_size, filename, 0)\n",
    "\t# generate translated images\n",
    "\tX_out, _ = generate_fake_samples(g_model, X_in, 0)\n",
    "\t# scale all pixels from [-1,1] to [0,1]\n",
    "\tX_in = (X_in + 1) / 2.0\n",
    "\tX_out = (X_out + 1) / 2.0\n",
    "\ta = np.zeros((n_samples, *(64,64,64,3)))\n",
    "\tfor i in range(n_samples):\n",
    "\t\t\ta[i,:,:,:,:2] = padding(X_in[i], (64,64,64,3))\n",
    "\tX_in = a\n",
    "\t#X_out = (X_out + 1) / 2.0\n",
    "\tif dataset == '/Masks/':\n",
    "\t\ta = np.zeros((n_samples, *(64,64,64,3)))\n",
    "\t\tfor i in range(n_samples):\n",
    "\t\t\t\ta[i,:,:,:,:2] = padding(X_out[i], (64,64,64,3))\n",
    "\t\tX_out = a\n",
    "\t# If its a segmentation mask we have to turn to RGB\n",
    "\tif dataset == '/Images/':\n",
    "\t\ta = np.empty((n_samples, *(64,64,64,3)))\n",
    "\t\tfor i in range(n_samples):\n",
    "\t\t\ta[i] = thresholding(X_out[i])\n",
    "\t\tX_out = a\n",
    "\t\t\n",
    "\n",
    "\t# plot real images\n",
    "\tfor i in range(n_samples):\n",
    "\t\tpyplot.subplot(2, n_samples, 1 + i)\n",
    "\t\tpyplot.axis('off')\n",
    "\t\tpyplot.imshow(X_in[i,:,:,25,:])\n",
    "\t# plot translated image\n",
    "\tfor i in range(n_samples):\n",
    "\t\tpyplot.subplot(2, n_samples, 1 + n_samples + i)\n",
    "\t\tpyplot.axis('off')\n",
    "\t\tpyplot.imshow(X_out[i,:,:,25,:])\n",
    "\t# save plot to file\n",
    "\tfilename1 = results_path + '%s_generated_plot_%06d.png' % (name, (step+1))\n",
    "\tpyplot.savefig(filename1)\n",
    "\tpyplot.close()\n",
    "\n",
    "# update image pool for fake images\n",
    "def update_image_pool(pool, images, max_size=50):\n",
    "\tselected = list()\n",
    "\tfor image in images:\n",
    "\t\tif len(pool) < max_size:\n",
    "\t\t\t# stock the pool\n",
    "\t\t\tpool.append(image)\n",
    "\t\t\tselected.append(image)\n",
    "\t\telif random() < 0.5:\n",
    "\t\t\t# use image, but don't add it to the pool\n",
    "\t\t\tselected.append(image)\n",
    "\t\telse:\n",
    "\t\t\t# replace an existing image and use replaced image\n",
    "\t\t\tix = randint(0, len(pool))\n",
    "\t\t\tselected.append(pool[ix])\n",
    "\t\t\tpool[ix] = image\n",
    "\treturn asarray(selected)\n",
    "\n",
    "# train cyclegan models\n",
    "def train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, filename, img_dim):\n",
    "    # Results\n",
    "    path = './Dataset/Results_CycleGAN/'\n",
    "    if os.path.exists(path)==False:\n",
    "        os.mkdir(path)\n",
    "    # define properties of the training run\n",
    "    n_epochs, n_batch = 100, 2\n",
    "    # determine the output square shape of the discriminator\n",
    "    n_patch = d_model_A.output_shape[1]\n",
    "    # unpack dataset\n",
    "    list_IDs = os.listdir(filename + '/Images/')\n",
    "    # prepare image pool for fakes\n",
    "    poolA, poolB = list(), list()\n",
    "    # calculate the number of batches per training epoch\n",
    "    bat_per_epo = int(len(list_IDs) / n_batch)\n",
    "    # calculate the number of training iterations\n",
    "    n_steps = bat_per_epo * n_epochs\n",
    "    e = 1\n",
    "    steps = 1\n",
    "    print('Epoch {}/{}'.format(e,n_epochs))\n",
    "    prev_g_loss1 = np.inf\n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_steps):\n",
    "\t\t# select a batch of real samples\n",
    "        X_realA, y_realA = generate_real_samples('/Images/', n_batch, img_dim, filename, n_patch)\n",
    "        X_realB, y_realB = generate_real_samples('/Masks/', n_batch, img_dim, filename, n_patch)\n",
    "        # generate a batch of fake samples\n",
    "        X_fakeA, y_fakeA = generate_fake_samples(g_model_BtoA, X_realB, n_patch)\n",
    "        X_fakeB, y_fakeB = generate_fake_samples(g_model_AtoB, X_realA, n_patch)\n",
    "        # update fakes from pool\n",
    "        X_fakeA = update_image_pool(poolA, X_fakeA)\n",
    "        X_fakeB = update_image_pool(poolB, X_fakeB)\n",
    "        # update generator B->A via adversarial and cycle loss\n",
    "        g_loss2, _, _, _, _  = c_model_BtoA.train_on_batch([X_realB, X_realA], [y_realA, X_realA, X_realB, X_realA])\n",
    "        # update discriminator for A -> [real/fake]\n",
    "        dA_loss1 = d_model_A.train_on_batch(X_realA, y_realA)\n",
    "        dA_loss2 = d_model_A.train_on_batch(X_fakeA, y_fakeA)\n",
    "        # update generator A->B via adversarial and cycle loss\n",
    "        g_loss1, _, _, _, _ = c_model_AtoB.train_on_batch([X_realA, X_realB], [y_realB, X_realB, X_realA, X_realB])\n",
    "        # update discriminator for B -> [real/fake]\n",
    "        dB_loss1 = d_model_B.train_on_batch(X_realB, y_realB)\n",
    "        dB_loss2 = d_model_B.train_on_batch(X_fakeB, y_fakeB)\n",
    "        # summarize performance\n",
    "        # print('>%d, dA[%.3f,%.3f] dB[%.3f,%.3f] g[%.3f,%.3f]' % (i+1, dA_loss1,dA_loss2, dB_loss1,dB_loss2, g_loss1,g_loss2))\n",
    "        stdout.write('\\rBatch: {}/{} - dA[{:.4f},{:.4f}] - dB[{:.4f},{:.4f}] - g[{:.4f},{:.4f}]         '\n",
    "                      .format(steps, bat_per_epo, dA_loss1,dA_loss2, dB_loss1,dB_loss2, g_loss1,g_loss2))\n",
    "        stdout.flush()\n",
    "        steps = steps + 1\n",
    "        \n",
    "        # evaluate the model performance every epoch and save the model if its better\n",
    "        if (i+1) % (bat_per_epo * 1) == 0:\n",
    "\n",
    "            #if g_loss1 < prev_g_loss1:\n",
    "            # save the models\n",
    "            save_models(i, g_model_AtoB, g_model_BtoA)\n",
    "            #prev_g_loss1 = g_loss1\n",
    "                \n",
    "            # plot A->B translation\n",
    "            summarize_performance(i, g_model_AtoB, '/Images/', 'AtoB', img_dim, path)\n",
    "            # plot B->A translation\n",
    "            summarize_performance(i, g_model_BtoA, '/Masks/', 'BtoA', img_dim, path)\n",
    "\n",
    "            steps = 1\n",
    "            e = e + 1\n",
    "            print('\\n Epoch {}/{} \\n'.format(e,n_epochs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rP0hB5gy5RA3",
    "tags": []
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10013,
     "status": "ok",
     "timestamp": 1655633630439,
     "user": {
      "displayName": "Alice Henriques da Rosa",
      "userId": "12243278505695630565"
     },
     "user_tz": -60
    },
    "id": "_VotxHp65QJ_",
    "outputId": "5594477f-8668-44bd-cd0b-d0b69c866d6a"
   },
   "outputs": [],
   "source": [
    "img_shape = (64,64,64,2)\n",
    "filename = './Dataset/Patches_64_cycleGAN/Train'\n",
    "# generator: A -> B\n",
    "g_model_AtoB = define_generator(img_shape)\n",
    "# generator: B -> A\n",
    "g_model_BtoA = define_generator(img_shape)\n",
    "# discriminator: A -> [real/fake]\n",
    "d_model_A = define_discriminator(img_shape)\n",
    "# discriminator: B -> [real/fake]\n",
    "d_model_B = define_discriminator(img_shape)\n",
    "\n",
    "# composite: A -> B -> [real/fake, A]\n",
    "c_model_AtoB = define_composite_model(g_model_AtoB, d_model_B, g_model_BtoA, img_shape)\n",
    "# composite: B -> A -> [real/fake, B]\n",
    "c_model_BtoA = define_composite_model(g_model_BtoA, d_model_A, g_model_AtoB, img_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fD6EpuDUTirj",
    "outputId": "319653ff-6c72-4838-f9ab-913bc57d1871",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-15 18:24:06.122568: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-10-15 18:24:06.881779: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8101\n",
      "2022-10-15 18:24:07.063594: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-10-15 18:24:07.063913: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-10-15 18:24:07.063935: W tensorflow/stream_executor/gpu/asm_compiler.cc:77] Couldn't get ptxas version string: Internal: Couldn't invoke ptxas --version\n",
      "2022-10-15 18:24:07.064332: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-10-15 18:24:07.064382: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2022-10-15 18:24:28.485809: W tensorflow/core/common_runtime/bfc_allocator.cc:272] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.50GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2022-10-15 18:24:28.729366: W tensorflow/core/common_runtime/bfc_allocator.cc:272] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.92GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 216/216 - dA[0.0513,0.1147] - dB[0.0711,0.1503] - g[79.5224,80.2427]          >Saved: ./Models_CycleGAN/g_model_AtoB_216.h5 and ./Models_CycleGAN/g_model_BtoA_216.h5 \n",
      "\n",
      "\n",
      " Epoch 2/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0197,0.0742] - dB[0.0378,0.1542] - g[68.4011,69.8147]         >Saved: ./Models_CycleGAN/g_model_AtoB_432.h5 and ./Models_CycleGAN/g_model_BtoA_432.h5 \n",
      "\n",
      "\n",
      " Epoch 3/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0356,0.0906] - dB[0.0121,0.0423] - g[58.8408,59.2179]         >Saved: ./Models_CycleGAN/g_model_AtoB_648.h5 and ./Models_CycleGAN/g_model_BtoA_648.h5 \n",
      "\n",
      "\n",
      " Epoch 4/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0215,0.0238] - dB[0.0874,0.0575] - g[52.1316,51.9291]         >Saved: ./Models_CycleGAN/g_model_AtoB_864.h5 and ./Models_CycleGAN/g_model_BtoA_864.h5 \n",
      "\n",
      "\n",
      " Epoch 5/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0446,0.0314] - dB[0.1008,0.1299] - g[43.4615,43.3553]         >Saved: ./Models_CycleGAN/g_model_AtoB_1080.h5 and ./Models_CycleGAN/g_model_BtoA_1080.h5 \n",
      "\n",
      "\n",
      " Epoch 6/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0301,0.0673] - dB[0.0175,0.0234] - g[40.3951,40.2225]         >Saved: ./Models_CycleGAN/g_model_AtoB_1296.h5 and ./Models_CycleGAN/g_model_BtoA_1296.h5 \n",
      "\n",
      "\n",
      " Epoch 7/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0138,0.0319] - dB[0.0149,0.0085] - g[35.1719,34.9820]         >Saved: ./Models_CycleGAN/g_model_AtoB_1512.h5 and ./Models_CycleGAN/g_model_BtoA_1512.h5 \n",
      "\n",
      "\n",
      " Epoch 8/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.1688,0.0883] - dB[0.0138,0.0249] - g[30.4893,30.6307]         >Saved: ./Models_CycleGAN/g_model_AtoB_1728.h5 and ./Models_CycleGAN/g_model_BtoA_1728.h5 \n",
      "\n",
      "\n",
      " Epoch 9/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0452,0.1281] - dB[0.0132,0.0436] - g[27.0499,27.5187]         >Saved: ./Models_CycleGAN/g_model_AtoB_1944.h5 and ./Models_CycleGAN/g_model_BtoA_1944.h5 \n",
      "\n",
      "\n",
      " Epoch 10/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0259,0.0202] - dB[0.0101,0.0080] - g[25.8708,26.1643]         >Saved: ./Models_CycleGAN/g_model_AtoB_2160.h5 and ./Models_CycleGAN/g_model_BtoA_2160.h5 \n",
      "\n",
      "\n",
      " Epoch 11/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.1439,0.1819] - dB[0.0395,0.0149] - g[22.7728,22.8696]         >Saved: ./Models_CycleGAN/g_model_AtoB_2376.h5 and ./Models_CycleGAN/g_model_BtoA_2376.h5 \n",
      "\n",
      "\n",
      " Epoch 12/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0647,0.0043] - dB[0.1714,0.0724] - g[21.7979,20.7144]         >Saved: ./Models_CycleGAN/g_model_AtoB_2592.h5 and ./Models_CycleGAN/g_model_BtoA_2592.h5 \n",
      "\n",
      "\n",
      " Epoch 13/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0101,0.0391] - dB[0.0589,0.1535] - g[16.1356,16.6386]         >Saved: ./Models_CycleGAN/g_model_AtoB_2808.h5 and ./Models_CycleGAN/g_model_BtoA_2808.h5 \n",
      "\n",
      "\n",
      " Epoch 14/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0253,0.0086] - dB[0.4570,0.2450] - g[15.2751,15.8773]         >Saved: ./Models_CycleGAN/g_model_AtoB_3024.h5 and ./Models_CycleGAN/g_model_BtoA_3024.h5 \n",
      "\n",
      "\n",
      " Epoch 15/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0053,0.0027] - dB[0.3528,0.3709] - g[13.3891,14.3926]         >Saved: ./Models_CycleGAN/g_model_AtoB_3240.h5 and ./Models_CycleGAN/g_model_BtoA_3240.h5 \n",
      "\n",
      "\n",
      " Epoch 16/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0259,0.0554] - dB[0.1155,0.0667] - g[11.8243,11.6829]         >Saved: ./Models_CycleGAN/g_model_AtoB_3456.h5 and ./Models_CycleGAN/g_model_BtoA_3456.h5 \n",
      "\n",
      "\n",
      " Epoch 17/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0075,0.0444] - dB[0.0091,0.0197] - g[12.7792,13.3317]         >Saved: ./Models_CycleGAN/g_model_AtoB_3672.h5 and ./Models_CycleGAN/g_model_BtoA_3672.h5 \n",
      "\n",
      "\n",
      " Epoch 18/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.2466,0.0203] - dB[0.2088,0.0746] - g[11.4153,11.4419]         >Saved: ./Models_CycleGAN/g_model_AtoB_3888.h5 and ./Models_CycleGAN/g_model_BtoA_3888.h5 \n",
      "\n",
      "\n",
      " Epoch 19/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0104,0.0112] - dB[0.1989,0.3389] - g[10.1321,11.3794]         >Saved: ./Models_CycleGAN/g_model_AtoB_4104.h5 and ./Models_CycleGAN/g_model_BtoA_4104.h5 \n",
      "\n",
      "\n",
      " Epoch 20/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0530,0.1148] - dB[0.2813,0.1499] - g[8.6647,8.4670]           >Saved: ./Models_CycleGAN/g_model_AtoB_4320.h5 and ./Models_CycleGAN/g_model_BtoA_4320.h5 \n",
      "\n",
      "\n",
      " Epoch 21/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0191,0.0472] - dB[0.1003,0.0610] - g[8.1764,8.3507]          >Saved: ./Models_CycleGAN/g_model_AtoB_4536.h5 and ./Models_CycleGAN/g_model_BtoA_4536.h5 \n",
      "\n",
      "\n",
      " Epoch 22/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.3965,0.0316] - dB[0.0635,0.0079] - g[7.5369,8.1887]         >Saved: ./Models_CycleGAN/g_model_AtoB_4752.h5 and ./Models_CycleGAN/g_model_BtoA_4752.h5 \n",
      "\n",
      "\n",
      " Epoch 23/100 \n",
      "\n",
      "Batch: 184/216 - dA[0.1112,0.0200] - dB[0.0081,0.0513] - g[7.5107,8.4263]          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 216/216 - dA[0.0253,0.0787] - dB[0.0887,0.1798] - g[6.3082,6.5707]         >Saved: ./Models_CycleGAN/g_model_AtoB_5184.h5 and ./Models_CycleGAN/g_model_BtoA_5184.h5 \n",
      "\n",
      "\n",
      " Epoch 25/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.1496,0.0803] - dB[0.0061,0.0253] - g[6.2879,6.4359]         >Saved: ./Models_CycleGAN/g_model_AtoB_5400.h5 and ./Models_CycleGAN/g_model_BtoA_5400.h5 \n",
      "\n",
      "\n",
      " Epoch 26/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.2224,0.4288] - dB[0.1200,0.0310] - g[5.5047,5.0917]         >Saved: ./Models_CycleGAN/g_model_AtoB_5616.h5 and ./Models_CycleGAN/g_model_BtoA_5616.h5 \n",
      "\n",
      "\n",
      " Epoch 27/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0116,0.0117] - dB[0.0361,0.0415] - g[4.6434,5.5461]           >Saved: ./Models_CycleGAN/g_model_AtoB_5832.h5 and ./Models_CycleGAN/g_model_BtoA_5832.h5 \n",
      "\n",
      "\n",
      " Epoch 28/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0099,0.0083] - dB[0.0152,0.0117] - g[5.4449,5.5949]          >Saved: ./Models_CycleGAN/g_model_AtoB_6048.h5 and ./Models_CycleGAN/g_model_BtoA_6048.h5 \n",
      "\n",
      "\n",
      " Epoch 29/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0137,0.0885] - dB[0.0191,0.1292] - g[4.9227,4.4686]          >Saved: ./Models_CycleGAN/g_model_AtoB_6264.h5 and ./Models_CycleGAN/g_model_BtoA_6264.h5 \n",
      "\n",
      "\n",
      " Epoch 30/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.1724,0.0851] - dB[0.0157,0.0641] - g[5.2083,5.7080]          >Saved: ./Models_CycleGAN/g_model_AtoB_6480.h5 and ./Models_CycleGAN/g_model_BtoA_6480.h5 \n",
      "\n",
      "\n",
      " Epoch 31/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0664,0.1287] - dB[0.0618,0.0853] - g[4.4931,4.9548]         >Saved: ./Models_CycleGAN/g_model_AtoB_6696.h5 and ./Models_CycleGAN/g_model_BtoA_6696.h5 \n",
      "\n",
      "\n",
      " Epoch 32/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0083,0.2079] - dB[0.0202,0.0063] - g[3.9403,3.9027]         >Saved: ./Models_CycleGAN/g_model_AtoB_6912.h5 and ./Models_CycleGAN/g_model_BtoA_6912.h5 \n",
      "\n",
      "\n",
      " Epoch 33/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0050,0.0389] - dB[0.0103,0.0054] - g[4.4678,4.0221]         >Saved: ./Models_CycleGAN/g_model_AtoB_7128.h5 and ./Models_CycleGAN/g_model_BtoA_7128.h5 \n",
      "\n",
      "\n",
      " Epoch 34/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0010,0.0018] - dB[0.0168,0.0859] - g[2.8937,3.4583]         >Saved: ./Models_CycleGAN/g_model_AtoB_7344.h5 and ./Models_CycleGAN/g_model_BtoA_7344.h5 \n",
      "\n",
      "\n",
      " Epoch 35/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0012,0.0078] - dB[0.0442,0.0070] - g[3.8828,4.0090]         >Saved: ./Models_CycleGAN/g_model_AtoB_7560.h5 and ./Models_CycleGAN/g_model_BtoA_7560.h5 \n",
      "\n",
      "\n",
      " Epoch 36/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0072,0.0016] - dB[0.0282,0.0073] - g[3.3054,3.2457]         >Saved: ./Models_CycleGAN/g_model_AtoB_7776.h5 and ./Models_CycleGAN/g_model_BtoA_7776.h5 \n",
      "\n",
      "\n",
      " Epoch 37/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0036,0.0057] - dB[0.0087,0.0357] - g[2.7957,3.2601]         >Saved: ./Models_CycleGAN/g_model_AtoB_7992.h5 and ./Models_CycleGAN/g_model_BtoA_7992.h5 \n",
      "\n",
      "\n",
      " Epoch 38/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0059,0.0144] - dB[0.1254,0.1716] - g[2.6226,2.7786]         >Saved: ./Models_CycleGAN/g_model_AtoB_8208.h5 and ./Models_CycleGAN/g_model_BtoA_8208.h5 \n",
      "\n",
      "\n",
      " Epoch 39/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0045,0.0093] - dB[0.0915,0.0673] - g[2.3150,2.9440]         >Saved: ./Models_CycleGAN/g_model_AtoB_8424.h5 and ./Models_CycleGAN/g_model_BtoA_8424.h5 \n",
      "\n",
      "\n",
      " Epoch 40/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0077,0.0204] - dB[0.1124,0.0808] - g[2.2991,2.7874]         >Saved: ./Models_CycleGAN/g_model_AtoB_8640.h5 and ./Models_CycleGAN/g_model_BtoA_8640.h5 \n",
      "\n",
      "\n",
      " Epoch 41/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0447,0.0252] - dB[0.0330,0.0942] - g[2.2789,3.5178]         >Saved: ./Models_CycleGAN/g_model_AtoB_8856.h5 and ./Models_CycleGAN/g_model_BtoA_8856.h5 \n",
      "\n",
      "\n",
      " Epoch 42/100 \n",
      "\n",
      "Batch: 5/216 - dA[0.0446,0.0676] - dB[0.0679,0.0035] - g[2.4871,2.3324]         "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# train models\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_model_A\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_model_B\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg_model_AtoB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg_model_BtoA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_model_AtoB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_model_BtoA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_shape\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, filename, img_dim)\u001b[0m\n\u001b[1;32m    173\u001b[0m X_fakeB \u001b[38;5;241m=\u001b[39m update_image_pool(poolB, X_fakeB)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# update generator B->A via adversarial and cycle loss\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m g_loss2, _, _, _, _  \u001b[38;5;241m=\u001b[39m \u001b[43mc_model_BtoA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_realB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_realA\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43my_realA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_realA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_realB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_realA\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;66;03m# update discriminator for A -> [real/fake]\u001b[39;00m\n\u001b[1;32m    177\u001b[0m dA_loss1 \u001b[38;5;241m=\u001b[39m d_model_A\u001b[38;5;241m.\u001b[39mtrain_on_batch(X_realA, y_realA)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/keras/engine/training.py:1856\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   1852\u001b[0m   iterator \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39msingle_batch_iterator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy, x,\n\u001b[1;32m   1853\u001b[0m                                                 y, sample_weight,\n\u001b[1;32m   1854\u001b[0m                                                 class_weight)\n\u001b[1;32m   1855\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_train_function()\n\u001b[0;32m-> 1856\u001b[0m   logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reset_metrics:\n\u001b[1;32m   1859\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_metrics()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:885\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    882\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 885\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    887\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    888\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:917\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    914\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    915\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 917\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    919\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    920\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    921\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py:3039\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3036\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   3037\u001b[0m   (graph_function,\n\u001b[1;32m   3038\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3039\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3040\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1963\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1960\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1961\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1962\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1963\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1964\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1965\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m     args,\n\u001b[1;32m   1967\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1968\u001b[0m     executing_eagerly)\n\u001b[1;32m   1969\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py:591\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    590\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 591\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    598\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    599\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    600\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    603\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    604\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 59\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     62\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train models\n",
    "train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, filename, img_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7S4NZZ5bwYtQ",
    "tags": []
   },
   "source": [
    "# Test Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 282,
     "status": "ok",
     "timestamp": 1655719013519,
     "user": {
      "displayName": "Alice Henriques da Rosa",
      "userId": "12243278505695630565"
     },
     "user_tz": -60
    },
    "id": "M78fwNkKwh4S"
   },
   "outputs": [],
   "source": [
    "def padding(image,size):\n",
    "\n",
    "    img_reshape = np.moveaxis(image, -1, 0)\n",
    "    \n",
    "    m = ReflectionPad3d((0,size[2]-image.shape[2],0,size[1]-image.shape[1],0,size[0]-image.shape[0]))\n",
    "    input = torch.tensor(img_reshape, dtype=torch.float)\n",
    "    output = m(input)\n",
    "    pad_img = output.numpy()\n",
    "    pad_img = np.moveaxis(pad_img, 0, -1)\n",
    "    \n",
    "    return pad_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1655719013828,
     "user": {
      "displayName": "Alice Henriques da Rosa",
      "userId": "12243278505695630565"
     },
     "user_tz": -60
    },
    "id": "E_EdHMzzwkEp"
   },
   "outputs": [],
   "source": [
    "def thresholding(patch_prediction):\n",
    "    patch_ind_nuclei = np.argwhere(patch_prediction[:,:,:,0] > 0.5)\n",
    "    patch_ind_golgi = np.argwhere(patch_prediction[:,:,:,1] > 0.5)\n",
    "\n",
    "    patch_prediction_thr = np.zeros((patch_prediction.shape[0],patch_prediction.shape[1],patch_prediction.shape[2],3))\n",
    "    #patch_prediction_thr_final = np.zeros((patch_prediction.shape[0],patch_prediction.shape[1],patch_prediction.shape[2],3))\n",
    "\n",
    "    for i in range(patch_ind_nuclei.shape[0]):\n",
    "        patch_prediction_thr[patch_ind_nuclei[i,0],patch_ind_nuclei[i,1],patch_ind_nuclei[i,2],0]=1 \n",
    "\n",
    "    for j in range(patch_ind_golgi.shape[0]):\n",
    "        patch_prediction_thr[patch_ind_golgi[j,0],patch_ind_golgi[j,1],patch_ind_golgi[j,2],1]=1\n",
    "    \n",
    "    #patch_prediction_thr_final[10:patch_prediction.shape[0]-10, 10:patch_prediction.shape[1]-10, 10:patch_prediction.shape[2]-10, :] = patch_prediction_thr[10:patch_prediction.shape[0]-10, 10:patch_prediction.shape[1]-10, 10:patch_prediction.shape[2]-10, :]\n",
    "\n",
    "    \n",
    "    return patch_prediction_thr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1655719013829,
     "user": {
      "displayName": "Alice Henriques da Rosa",
      "userId": "12243278505695630565"
     },
     "user_tz": -60
    },
    "id": "kFSYuLBPwmBc"
   },
   "outputs": [],
   "source": [
    "def pred_mask(image, mask_shape):\n",
    "    patch_size = 64\n",
    "    step = 48\n",
    "\n",
    "    pred_mask = np.zeros(((image.shape[0] // step)*step + patch_size,(image.shape[1] // step)*step + patch_size,64,3))\n",
    "    _image = padding(image, ((image.shape[0] // step)*step + patch_size,(image.shape[1] // step)*step + patch_size,64,image.shape[3]))\n",
    "\n",
    "    i = 0\n",
    "    while i + patch_size <= _image.shape[0]:\n",
    "        j = 0\n",
    "        while j + patch_size <= _image.shape[1]:\n",
    "\n",
    "            tst_patch = _image[i:i+patch_size, j:j+patch_size, :, :]\n",
    "            tst_patch = np.array([(tst_patch - 127.5) / 127.5])\n",
    "            preds_tst = my_model.predict(tst_patch)\n",
    "            preds_tst = (preds_tst + 1) / 2.0\n",
    "            pred_patch = preds_tst[0,:,:,:,:]\n",
    "            pred_patch_thr = thresholding(pred_patch)\n",
    "\n",
    "            pred_mask[i:i+patch_size, j:j+patch_size,:,0] = np.array(np.logical_or(pred_mask[i:i+patch_size, j:j+patch_size, :,0], pred_patch_thr[:,:,:,0], dtype = 'float64'))\n",
    "            pred_mask[i:i+patch_size, j:j+patch_size,:,1] = np.array(np.logical_or(pred_mask[i:i+patch_size, j:j+patch_size, :,1], pred_patch_thr[:,:,:,1], dtype = 'float64'))\n",
    "            pred_mask[i:i+patch_size, j:j+patch_size,:,2] = 0\n",
    "\n",
    "            j += step\n",
    "\n",
    "        i += step\n",
    "\n",
    "    _pred_mask = np.zeros(mask_shape)\n",
    "    _pred_mask = pred_mask[0:mask_shape[0],0:mask_shape[1],0:mask_shape[2],:]\n",
    "\n",
    "    return _pred_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 262,
     "status": "ok",
     "timestamp": 1655719014087,
     "user": {
      "displayName": "Alice Henriques da Rosa",
      "userId": "12243278505695630565"
     },
     "user_tz": -60
    },
    "id": "_Ny5V_2WwoJ6"
   },
   "outputs": [],
   "source": [
    "def dice_coefficient(y_true, y_pred):\n",
    "    smoothing_factor = 1\n",
    "    flat_y_true = K.flatten(y_true)\n",
    "    flat_y_pred = K.flatten(y_pred)\n",
    "    return (2. * K.sum(flat_y_true * flat_y_pred) + smoothing_factor) / (K.sum(flat_y_true) + K.sum(flat_y_pred) + smoothing_factor)\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    flat_y_true = K.flatten(y_true)\n",
    "    flat_y_pred = K.flatten(y_pred)\n",
    "    return(K.sum(flat_y_true * flat_y_pred) / K.sum(flat_y_pred) )\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    flat_y_true = K.flatten(y_true)\n",
    "    flat_y_pred = K.flatten(y_pred)\n",
    "    return(K.sum(flat_y_true * flat_y_pred) / K.sum(flat_y_true) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1655719014089,
     "user": {
      "displayName": "Alice Henriques da Rosa",
      "userId": "12243278505695630565"
     },
     "user_tz": -60
    },
    "id": "JkqIRTBAwp47"
   },
   "outputs": [],
   "source": [
    "def metrics(mask, _pred_mask):\n",
    "    _mask = mask/255.\n",
    "\n",
    "    dice_coef_nuclei = dice_coefficient(_mask[:,:,:,1],_pred_mask[:,:,:,1])\n",
    "    dice_coef_golgi = dice_coefficient(_mask[:,:,:,0],_pred_mask[:,:,:,0])\n",
    "    prec_nuclei = precision(_mask[:,:,:,1],_pred_mask[:,:,:,1])\n",
    "    prec_golgi = precision(_mask[:,:,:,0],_pred_mask[:,:,:,0])\n",
    "    recall_nuclei = recall(_mask[:,:,:,1],_pred_mask[:,:,:,1])\n",
    "    recall_golgi = recall(_mask[:,:,:,0],_pred_mask[:,:,:,0])\n",
    "\n",
    "    return [round(dice_coef_nuclei.numpy(),4), round(dice_coef_golgi.numpy(),4), \n",
    "          round(prec_nuclei.numpy(),4), round(prec_golgi.numpy(),4), round(recall_nuclei.numpy(),4),\n",
    "          round(recall_golgi.numpy(),4)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1655719014090,
     "user": {
      "displayName": "Alice Henriques da Rosa",
      "userId": "12243278505695630565"
     },
     "user_tz": -60
    },
    "id": "opglOpuzwsIm"
   },
   "outputs": [],
   "source": [
    "def write_to_excel(wb,sheet_name,metrics, result_dir):\n",
    "\n",
    "    # add_sheet is used to create sheet.\n",
    "    sheet1 = wb.add_sheet(sheet_name)\n",
    "\n",
    "    sheet1.write(0, 0, 'Dice Coeffient Nuclei')\n",
    "    sheet1.write(0, 1, 'Dice Coeffient Golgi')\n",
    "    sheet1.write(0, 2, 'Precision Nuclei')\n",
    "    sheet1.write(0, 3, 'Precision Golgi')\n",
    "    sheet1.write(0, 4, 'Recall Nuclei')\n",
    "    sheet1.write(0, 5, 'Recall Golgi')\n",
    "    sheet1.write(1, 0, metrics[0])\n",
    "    sheet1.write(1, 1, metrics[1])\n",
    "    sheet1.write(1, 2, metrics[2])\n",
    "    sheet1.write(1, 3, metrics[3])\n",
    "    sheet1.write(1, 4, metrics[4])\n",
    "    sheet1.write(1, 5, metrics[5])\n",
    "\n",
    "    wb.save(result_dir + '/results_metrics.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1655719014090,
     "user": {
      "displayName": "Alice Henriques da Rosa",
      "userId": "12243278505695630565"
     },
     "user_tz": -60
    },
    "id": "8yDXNAWGwviV"
   },
   "outputs": [],
   "source": [
    "def test_model(base_dir):\n",
    "\n",
    "    fnames = os.listdir(base_dir + 'Images/')\n",
    "\n",
    "    # Workbook is created\n",
    "    wb = Workbook()\n",
    "\n",
    "    result_dir = './Dataset/Results_CycleGAN'\n",
    "    if os.path.exists(result_dir)==False:\n",
    "        os.mkdir(result_dir)\n",
    "\n",
    "    for i in range(len(fnames)):\n",
    "        \n",
    "        image = imread(base_dir + 'Images/' + fnames[i])[:,:,:,:2]\n",
    "        mask = imread(base_dir + 'Masks/' + fnames[i])\n",
    "\n",
    "        predicted_mask = pred_mask(image, image.shape)\n",
    "\n",
    "        _predicted_mask = predicted_mask*255.0\n",
    "        _predicted_mask = _predicted_mask.astype('uint8')\n",
    "\n",
    "        imwrite(result_dir + '/pred_mask_' + fnames[i] , _predicted_mask, photometric='rgb')\n",
    "\n",
    "        _metrics = metrics(mask, predicted_mask)\n",
    "\n",
    "        write_to_excel(wb,'Sheet_' + fnames[i].split('.')[0], _metrics, result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 136212,
     "status": "ok",
     "timestamp": 1655719184999,
     "user": {
      "displayName": "Alice Henriques da Rosa",
      "userId": "12243278505695630565"
     },
     "user_tz": -60
    },
    "id": "8Uk3-amtwwMW",
    "outputId": "635ef0c8-f8e9-46db-a347-cbf291a61844"
   },
   "outputs": [],
   "source": [
    "#Load the pretrained model for testing and predictions. \n",
    "\n",
    "#Load the pretrained model for testing and predictions. \n",
    "from keras.models import load_model\n",
    "my_model = load_model('./Models_CycleGAN/g_model_AtoB_8640.h5', compile=False, custom_objects={'InstanceNormalization':InstanceNormalization})\n",
    "#If you load a different model do not forget to preprocess accordingly. \n",
    "\n",
    "base_dir = './Dataset/Patches_64_cycleGAN/Test/'\n",
    "\n",
    "test_model(base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgOh29oJMzW3",
    "tags": []
   },
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(filename, dataset):\n",
    "    \n",
    "    list_IDs_mask = os.listdir(filename + dataset)\n",
    "    list_IDs_images = os.listdir(filename + '/Images/')\n",
    "    i = 0\n",
    "    j = 0\n",
    "\n",
    "    for ID in list_IDs_mask:\n",
    "        X = imread(filename + dataset + ID)[:,:,:,0]\n",
    "        \n",
    "        is_all_zeros = np.all((X == 0))\n",
    "        \n",
    "        if is_all_zeros:\n",
    "            #os.remove(filename + dataset + ID)\n",
    "            print(\"File\",ID,\"deleted mask\")\n",
    "            j = j+1\n",
    "            \n",
    "    for ID in list_IDs_images:\n",
    "        y = imread(filename + '/Images/' + ID)[:,:,:,0]\n",
    "        is_all_low = np.all((y <= (200/255.)))\n",
    "            \n",
    "        if is_all_low:\n",
    "            #os.remove(filename + '/Images/' + ID)\n",
    "            print(\"File\",ID,\"deleted image\")\n",
    "            i = i+1\n",
    "            \n",
    "    print(len(list_IDs_mask), j)\n",
    "    print(len(list_IDs_images), i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "433\n",
      "433\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#pre_processing('./Dataset/Patches_synthetic_64/Train', '/Masks/')\n",
    "list_IDs = os.listdir('./Dataset/Patches_synthetic_64/Train' + '/Masks/')\n",
    "print(len(list_IDs))\n",
    "list_IDs_ = os.listdir('./Dataset/Patches_synthetic_64/Train' + '/Images/')\n",
    "print(len(list_IDs_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f94edb7c310>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = imread('./Dataset/Patches_synthetic/Train' + '/Images/' + 'img_patch_110.tif')\n",
    "plt.figure(1)\n",
    "imshow(a[:,:,30,:])\n",
    "c = imread('./Dataset/Patches_synthetic/Train' + '/Masks/' + 'img_patch_110.tif')\n",
    "b = np.zeros((64,64,64,3))\n",
    "b[:,:,:,:2] = c\n",
    "plt.figure(2)\n",
    "imshow(b[:,:,30,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_patches = 37\n",
    "\n",
    "for i in range(n_patches):\n",
    "    patch = np.zeros((64,64,64,2), dtype='float32')\n",
    "    imwrite('./Dataset/Patches_synthetic/Train/Masks' + '/img_patch_' + str(7) + str(i) + '.tif', patch)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNsBO+gt9lsQYL2a5E+6P0i",
   "collapsed_sections": [
    "fgOh29oJMzW3"
   ],
   "name": "CycleGAN_V3.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
