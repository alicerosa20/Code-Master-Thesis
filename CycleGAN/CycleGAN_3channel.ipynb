{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "anSn07EhuGES",
    "tags": []
   },
   "source": [
    "# Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2758,
     "status": "ok",
     "timestamp": 1655719006528,
     "user": {
      "displayName": "Alice Henriques da Rosa",
      "userId": "12243278505695630565"
     },
     "user_tz": -60
    },
    "id": "Kb0w8OA2tzMj"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.8.0 and strictly below 2.11.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.6.2 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# example of training a cyclegan on the horse2zebra dataset\n",
    "from random import random\n",
    "import numpy as np\n",
    "from numpy import load\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy import asarray\n",
    "from numpy.random import randint\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.models import Model\n",
    "from keras.models import Input\n",
    "from keras.layers import Conv3D, BatchNormalization\n",
    "from keras.layers import Conv3DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import UpSampling3D\n",
    "from keras.activations import sigmoid\n",
    "from matplotlib import pyplot\n",
    "import os\n",
    "from tifffile import imread, imsave, imwrite\n",
    "from sys import stdout\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "from keras import backend as K\n",
    "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow_addons.layers import SpectralNormalization\n",
    "import xlwt\n",
    "from xlwt import Workbook\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.nn import ReflectionPad3d\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_TQLQxzuJIz",
    "tags": []
   },
   "source": [
    "# Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SW_t-_lNt2jy"
   },
   "outputs": [],
   "source": [
    "# define the discriminator model\n",
    "def define_discriminator(image_shape):\n",
    "    # weight initialization\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    # source image input\n",
    "    in_image = Input(shape=image_shape)\n",
    "\n",
    "    # C64\n",
    "    d = Conv3D(64, 4, strides=2, padding='same', kernel_initializer=init)(in_image)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # C128\n",
    "    d = Conv3D(128, 4, strides=2, padding='same', kernel_initializer=init)(d)\n",
    "    d = InstanceNormalization(axis=-1)(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # C256\n",
    "    d = Conv3D(256, 4, strides=2, padding='same', kernel_initializer=init)(d)\n",
    "    d = InstanceNormalization(axis=-1)(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # C512\n",
    "    d = Conv3D(512, 4, strides=2, padding='same', kernel_initializer=init)(d)\n",
    "    d = InstanceNormalization(axis=-1)(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # second last output layer\n",
    "    d = Conv3D(512, 4, padding='same', kernel_initializer=init)(d)\n",
    "    d = InstanceNormalization(axis=-1)(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # patch output\n",
    "    patch_out = Conv3D(1, 4, padding='same', kernel_initializer=init)(d)\n",
    "    # define model\n",
    "    model = Model(in_image, patch_out)\n",
    "    # compile model\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=0.0001, beta_1=0.5), loss_weights=[0.5])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9ylsvemot8OV"
   },
   "outputs": [],
   "source": [
    "# generator a resnet block\n",
    "def resnet_block(n_filters, input_layer):\n",
    "    # weight initialization\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    # first layer convolutional layer\n",
    "    g = Conv3D(n_filters, 3, padding='same', kernel_initializer=init)(input_layer)\n",
    "    g = InstanceNormalization(axis=-1)(g)\n",
    "    g = Activation('relu')(g)\n",
    "    # second convolutional layer\n",
    "    g = Conv3D(n_filters, 3, padding='same', kernel_initializer=init)(g)\n",
    "    g = InstanceNormalization(axis=-1)(g)\n",
    "    # concatenate merge channel-wise with input layer\n",
    "    g = Concatenate()([g, input_layer])\n",
    "    return g\n",
    "\n",
    "# define the standalone generator model\n",
    "def define_generator(image_shape, n_resnet=9):\n",
    "\n",
    "\t# weight initialization\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    # image input\n",
    "    in_image = Input(shape=image_shape)\n",
    "    # c7s1-32\n",
    "    g = Conv3D(32, 7, padding='same', kernel_initializer=init)(in_image)\n",
    "    g = InstanceNormalization(axis=-1)(g)\n",
    "    g = Activation('relu')(g)\n",
    "    # d64\n",
    "    g = Conv3D(64, 3, strides=2, padding='same', kernel_initializer=init)(g)\n",
    "    g = InstanceNormalization(axis=-1)(g)\n",
    "    g = Activation('relu')(g)\n",
    "    # d128\n",
    "    g = Conv3D(128, 3, strides=2, padding='same', kernel_initializer=init)(g)\n",
    "    g = InstanceNormalization(axis=-1)(g)\n",
    "    g = Activation('relu')(g)\n",
    "    # R256\n",
    "    for _ in range(n_resnet):\n",
    "        g = resnet_block(128, g)\n",
    "    # u64\n",
    "    g = Conv3DTranspose(64, 3, strides=2, padding='same', kernel_initializer=init)(g)\n",
    "    g = InstanceNormalization(axis=-1)(g)\n",
    "    g = Activation('relu')(g)\n",
    "    # u32\n",
    "    g = Conv3DTranspose(32, 3, strides=2, padding='same', kernel_initializer=init)(g)\n",
    "    g = InstanceNormalization(axis=-1)(g)\n",
    "    g = Activation('relu')(g)\n",
    "    # c7s1-2\n",
    "    g = Conv3D(3, 7, padding='same', kernel_initializer=init)(g)\n",
    "    g = InstanceNormalization(axis=-1)(g)\n",
    "    out_image = Activation('tanh')(g)\n",
    "    # define model\n",
    "    model = Model(in_image, out_image)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6oZZS6Gbt_hX"
   },
   "outputs": [],
   "source": [
    "def weighted_mean_absolute_error(class_weights):\n",
    "    def loss(y_true, y_pred):\n",
    "        mae = tf.keras.losses.MeanAbsoluteError()\n",
    "        red_channel_loss = mae(y_true[:,:,:,:,0],y_pred[:,:,:,:,0])\n",
    "        green_channel_loss = mae(y_true[:,:,:,:,1],y_pred[:,:,:,:,1])\n",
    "        blue_channel_loss = mae(y_true[:,:,:,:,2],y_pred[:,:,:,:,2])\n",
    "        return class_weights[0]*red_channel_loss+class_weights[1]*green_channel_loss+class_weights[2]*blue_channel_loss\n",
    "    return loss\n",
    "\n",
    "# define a composite model for updating generators by adversarial and cycle loss\n",
    "def define_composite_model(g_model_1, d_model, g_model_2, image_shape, class_weights=[3.5,1,1]):\n",
    "    # ensure the model we're updating is trainable\n",
    "    g_model_1.trainable = True\n",
    "    # mark discriminator as not trainable\n",
    "    d_model.trainable = False\n",
    "    # mark other generator model as not trainable\n",
    "    g_model_2.trainable = False\n",
    "    # discriminator element\n",
    "    input_gen = Input(shape=image_shape)\n",
    "    gen1_out = g_model_1(input_gen)\n",
    "    output_d = d_model(gen1_out)\n",
    "    # identity element\n",
    "    input_id = Input(shape=image_shape)\n",
    "    output_id = g_model_1(input_id)\n",
    "    # forward cycle\n",
    "    output_f = g_model_2(gen1_out)\n",
    "    # backward cycle\n",
    "    gen2_out = g_model_2(input_id)\n",
    "    output_b = g_model_1(gen2_out)\n",
    "    # define model graph\n",
    "    model = Model([input_gen, input_id], [output_d, output_id, output_f, output_b])\n",
    "    # define optimization algorithm configuration\n",
    "    opt = Adam(learning_rate=0.0005, beta_1=0.5)\n",
    "    # compile model with weighting of least squares loss and L1 loss\n",
    "    wmae = weighted_mean_absolute_error(class_weights)\n",
    "\n",
    "    model.compile(loss=['mse', wmae, wmae, wmae], loss_weights=[1, 5, 10, 10], optimizer=opt)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GcZ9nLp6uMAQ",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "UoX78GBw46io"
   },
   "outputs": [],
   "source": [
    "# load and prepare training images\n",
    "def load_real_samples(dataset, ID, filename, dim):\n",
    "    # load the dataset\n",
    "    if dataset == '/Images/':\n",
    "        dim = (64,64,64,3)\n",
    "    X = np.empty((len(ID), *dim))\n",
    "\n",
    "    for i, ID_path in enumerate(ID):\n",
    "        X[i,] = imread(filename + dataset + ID_path)\n",
    "        X[i,] = (X[i,] - 0.5) / 0.5\n",
    "    \n",
    "    return X\n",
    "\n",
    "\n",
    "# select a batch of random samples, returns images and target\n",
    "def generate_real_samples(dataset, n_samples, patch_size, filename, n_patch):\n",
    "\n",
    "\tlist_IDs = os.listdir(filename + dataset)\n",
    "\t# choose random instances\n",
    "\tindexes = randint(0, len(list_IDs), n_samples)\n",
    "\tlist_IDs_temp = [list_IDs[k] for k in indexes]\n",
    "\t# retrieve selected images\n",
    "\tX = load_real_samples(dataset, list_IDs_temp, filename, patch_size)\n",
    "\t# generate 'real' class labels (1)\n",
    "\ty = ones((n_samples, n_patch, n_patch, 1))\n",
    "\n",
    "\treturn X, y\n",
    "\n",
    "# generate a batch of images, returns images and targets\n",
    "def generate_fake_samples(g_model, dataset, patch_shape):\n",
    "\t# generate fake instance\n",
    "\tX = g_model.predict(dataset)\n",
    "\t# create 'fake' class labels (0)\n",
    "\ty = zeros((len(X), patch_shape, patch_shape, 1))\n",
    "\treturn X, y\n",
    "\n",
    "# save the generator models to file\n",
    "def save_models(step, g_model_AtoB, g_model_BtoA):\n",
    "\t# path to save models\n",
    "\tpath_models = './Models_CycleGAN_3/'\n",
    "\tif os.path.exists(path_models)==False:\n",
    "\t\tos.mkdir(path_models)\n",
    "\t# save the first generator model\n",
    "\tfilename1 = path_models + 'g_model_AtoB_%03d.h5' % (step+1)\n",
    "\tg_model_AtoB.save(filename1)\n",
    "\t# save the second generator model\n",
    "\tfilename2 = path_models + 'g_model_BtoA_%03d.h5' % (step+1)\n",
    "\tg_model_BtoA.save(filename2)\n",
    "\tprint('>Saved: %s and %s \\n' % (filename1, filename2))\n",
    " \n",
    "\n",
    "def thresholding(patch_prediction):\n",
    "    patch_ind_nuclei = np.argwhere(patch_prediction[:,:,:,0] > 0.5)\n",
    "    patch_ind_golgi = np.argwhere(patch_prediction[:,:,:,1] > 0.5)\n",
    "    patch_ind_int = np.argwhere(patch_prediction[:,:,:,2] > 0.5)\n",
    "\n",
    "    patch_prediction_thr = np.zeros((patch_prediction.shape[0],patch_prediction.shape[1],patch_prediction.shape[2],3))\n",
    "\n",
    "    for i in range(patch_ind_nuclei.shape[0]):\n",
    "      patch_prediction_thr[patch_ind_nuclei[i,0],patch_ind_nuclei[i,1],patch_ind_nuclei[i,2],0]=1 \n",
    "\n",
    "    for j in range(patch_ind_golgi.shape[0]):\n",
    "      patch_prediction_thr[patch_ind_golgi[j,0],patch_ind_golgi[j,1],patch_ind_golgi[j,2],1]=1\n",
    "    \n",
    "    for k in range(patch_ind_int.shape[0]):\n",
    "      patch_prediction_thr[patch_ind_int[k,0],patch_ind_int[k,1],patch_ind_int[k,2],2]=1\n",
    "\n",
    "    return patch_prediction_thr\n",
    "\n",
    "def padding(image,size):\n",
    "    \n",
    "    img_reshape = np.moveaxis(image, -1, 0)\n",
    "    \n",
    "    m = ReflectionPad3d((0,size[2]-image.shape[2],0,size[1]-image.shape[1],0,size[0]-image.shape[0]))\n",
    "    _input = torch.tensor(img_reshape, dtype=torch.float)\n",
    "    output = m(_input)\n",
    "    pad_img = output.numpy()\n",
    "    pad_img = np.moveaxis(pad_img, 0, -1)\n",
    "    \n",
    "    return pad_img\n",
    "\n",
    "# generate samples and save as a plot and save the model\n",
    "def summarize_performance(step, g_model, dataset, name, patch_size, results_path, n_samples=5, filename= './Dataset_3/Patches_synthetic_64/Train'):\n",
    "\t# select a sample of input images\n",
    "\tX_in, _ = generate_real_samples(dataset, n_samples, patch_size, filename, 0)\n",
    "\t# generate translated images\n",
    "\tX_out, _ = generate_fake_samples(g_model, X_in, 0)\n",
    "\t# scale all pixels from [-1,1] to [0,1]\n",
    "\tX_in = (X_in + 1) / 2.0\n",
    "\tX_out = (X_out + 1) / 2.0\n",
    "\ta = np.zeros((n_samples, *(64,64,64,3)))\n",
    "\tfor i in range(n_samples):\n",
    "\t\t\ta[i] = padding(X_in[i], (64,64,64,3))\n",
    "\tX_in = a\n",
    "\t#X_out = (X_out + 1) / 2.0\n",
    "\tif dataset == '/Masks/':\n",
    "\t\ta = np.zeros((n_samples, *(64,64,64,3)))\n",
    "\t\tfor i in range(n_samples):\n",
    "\t\t\t\ta[i] = padding(X_out[i], (64,64,64,3))\n",
    "\t\tX_out = a\n",
    "\t# If its a segmentation mask we have to turn to RGB\n",
    "\tif dataset == '/Images/':\n",
    "\t\ta = np.empty((n_samples, *(64,64,64,3)))\n",
    "\t\tfor i in range(n_samples):\n",
    "\t\t\ta[i] = thresholding(X_out[i])\n",
    "\t\tX_out = a\n",
    "\t\t\n",
    "\n",
    "\t# plot real images\n",
    "\tfor i in range(n_samples):\n",
    "\t\tpyplot.subplot(2, n_samples, 1 + i)\n",
    "\t\tpyplot.axis('off')\n",
    "\t\tpyplot.imshow(X_in[i,:,:,25,:])\n",
    "\t# plot translated image\n",
    "\tfor i in range(n_samples):\n",
    "\t\tpyplot.subplot(2, n_samples, 1 + n_samples + i)\n",
    "\t\tpyplot.axis('off')\n",
    "\t\tpyplot.imshow(X_out[i,:,:,25,:])\n",
    "\t# save plot to file\n",
    "\tfilename1 = results_path + '%s_generated_plot_%06d.png' % (name, (step+1))\n",
    "\tpyplot.savefig(filename1)\n",
    "\tpyplot.close()\n",
    "\n",
    "# update image pool for fake images\n",
    "def update_image_pool(pool, images, max_size=50):\n",
    "\tselected = list()\n",
    "\tfor image in images:\n",
    "\t\tif len(pool) < max_size:\n",
    "\t\t\t# stock the pool\n",
    "\t\t\tpool.append(image)\n",
    "\t\t\tselected.append(image)\n",
    "\t\telif random() < 0.5:\n",
    "\t\t\t# use image, but don't add it to the pool\n",
    "\t\t\tselected.append(image)\n",
    "\t\telse:\n",
    "\t\t\t# replace an existing image and use replaced image\n",
    "\t\t\tix = randint(0, len(pool))\n",
    "\t\t\tselected.append(pool[ix])\n",
    "\t\t\tpool[ix] = image\n",
    "\treturn asarray(selected)\n",
    "\n",
    "# train cyclegan models\n",
    "def train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, filename, img_dim):\n",
    "    # Results\n",
    "    path = './Dataset_3/Results_CycleGAN/'\n",
    "    if os.path.exists(path)==False:\n",
    "        os.mkdir(path)\n",
    "    # define properties of the training run\n",
    "    n_epochs, n_batch = 100, 2\n",
    "    # determine the output square shape of the discriminator\n",
    "    n_patch = d_model_A.output_shape[1]\n",
    "    # unpack dataset\n",
    "    list_IDs = os.listdir(filename + '/Images/')\n",
    "    # prepare image pool for fakes\n",
    "    poolA, poolB = list(), list()\n",
    "    # calculate the number of batches per training epoch\n",
    "    bat_per_epo = int(len(list_IDs) / n_batch)\n",
    "    # calculate the number of training iterations\n",
    "    n_steps = bat_per_epo * n_epochs\n",
    "    e = 1\n",
    "    steps = 1\n",
    "    print('Epoch {}/{}'.format(e,n_epochs))\n",
    "    prev_g_loss1 = np.inf\n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_steps):\n",
    "\t\t# select a batch of real samples\n",
    "        X_realA, y_realA = generate_real_samples('/Images/', n_batch, img_dim, filename, n_patch)\n",
    "        X_realB, y_realB = generate_real_samples('/Masks/', n_batch, img_dim, filename, n_patch)\n",
    "        # generate a batch of fake samples\n",
    "        X_fakeA, y_fakeA = generate_fake_samples(g_model_BtoA, X_realB, n_patch)\n",
    "        X_fakeB, y_fakeB = generate_fake_samples(g_model_AtoB, X_realA, n_patch)\n",
    "        # update fakes from pool\n",
    "        X_fakeA = update_image_pool(poolA, X_fakeA)\n",
    "        X_fakeB = update_image_pool(poolB, X_fakeB)\n",
    "        # update generator B->A via adversarial and cycle loss\n",
    "        g_loss2, _, _, _, _  = c_model_BtoA.train_on_batch([X_realB, X_realA], [y_realA, X_realA, X_realB, X_realA])\n",
    "        # update discriminator for A -> [real/fake]\n",
    "        dA_loss1 = d_model_A.train_on_batch(X_realA, y_realA)\n",
    "        dA_loss2 = d_model_A.train_on_batch(X_fakeA, y_fakeA)\n",
    "        # update generator A->B via adversarial and cycle loss\n",
    "        g_loss1, _, _, _, _ = c_model_AtoB.train_on_batch([X_realA, X_realB], [y_realB, X_realB, X_realA, X_realB])\n",
    "        # update discriminator for B -> [real/fake]\n",
    "        dB_loss1 = d_model_B.train_on_batch(X_realB, y_realB)\n",
    "        dB_loss2 = d_model_B.train_on_batch(X_fakeB, y_fakeB)\n",
    "        # summarize performance\n",
    "        # print('>%d, dA[%.3f,%.3f] dB[%.3f,%.3f] g[%.3f,%.3f]' % (i+1, dA_loss1,dA_loss2, dB_loss1,dB_loss2, g_loss1,g_loss2))\n",
    "        stdout.write('\\rBatch: {}/{} - dA[{:.4f},{:.4f}] - dB[{:.4f},{:.4f}] - g[{:.4f},{:.4f}]         '\n",
    "                      .format(steps, bat_per_epo, dA_loss1,dA_loss2, dB_loss1,dB_loss2, g_loss1,g_loss2))\n",
    "        stdout.flush()\n",
    "        steps = steps + 1\n",
    "        \n",
    "        # evaluate the model performance every epoch and save the model if its better\n",
    "        if (i+1) % (bat_per_epo * 1) == 0:\n",
    "\n",
    "            #if g_loss1 < prev_g_loss1:\n",
    "            # save the models\n",
    "            save_models(i, g_model_AtoB, g_model_BtoA)\n",
    "            #prev_g_loss1 = g_loss1\n",
    "                \n",
    "            # plot A->B translation\n",
    "            summarize_performance(i, g_model_AtoB, '/Images/', 'AtoB', img_dim, path)\n",
    "            # plot B->A translation\n",
    "            summarize_performance(i, g_model_BtoA, '/Masks/', 'BtoA', img_dim, path)\n",
    "\n",
    "            steps = 1\n",
    "            e = e + 1\n",
    "            print('\\n Epoch {}/{} \\n'.format(e,n_epochs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rP0hB5gy5RA3",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10013,
     "status": "ok",
     "timestamp": 1655633630439,
     "user": {
      "displayName": "Alice Henriques da Rosa",
      "userId": "12243278505695630565"
     },
     "user_tz": -60
    },
    "id": "_VotxHp65QJ_",
    "outputId": "5594477f-8668-44bd-cd0b-d0b69c866d6a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-14 07:59:10.691242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-14 07:59:10.696241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-14 07:59:10.696503: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-14 07:59:10.696837: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 07:59:10.697175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-14 07:59:10.697411: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-14 07:59:10.697546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-14 07:59:11.084413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-14 07:59:11.084574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-14 07:59:11.084759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-14 07:59:11.084858: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7101 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "img_shape = (64,64,64,3)\n",
    "filename = './Dataset_3/Patches_synthetic_64/Train'\n",
    "# generator: A -> B\n",
    "g_model_AtoB = define_generator(img_shape)\n",
    "# generator: B -> A\n",
    "g_model_BtoA = define_generator(img_shape)\n",
    "# discriminator: A -> [real/fake]\n",
    "d_model_A = define_discriminator(img_shape)\n",
    "# discriminator: B -> [real/fake]\n",
    "d_model_B = define_discriminator(img_shape)\n",
    "\n",
    "# composite: A -> B -> [real/fake, A]\n",
    "c_model_AtoB = define_composite_model(g_model_AtoB, d_model_B, g_model_BtoA, img_shape)\n",
    "# composite: B -> A -> [real/fake, B]\n",
    "c_model_BtoA = define_composite_model(g_model_BtoA, d_model_A, g_model_AtoB, img_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fD6EpuDUTirj",
    "outputId": "319653ff-6c72-4838-f9ab-913bc57d1871"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-14 07:59:14.933655: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-10-14 07:59:15.680989: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8101\n",
      "2022-10-14 07:59:15.858859: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-10-14 07:59:15.859199: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-10-14 07:59:15.859221: W tensorflow/stream_executor/gpu/asm_compiler.cc:77] Couldn't get ptxas version string: Internal: Couldn't invoke ptxas --version\n",
      "2022-10-14 07:59:15.859556: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-10-14 07:59:15.859600: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2022-10-14 07:59:37.483101: W tensorflow/core/common_runtime/bfc_allocator.cc:272] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.50GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2022-10-14 07:59:37.727092: W tensorflow/core/common_runtime/bfc_allocator.cc:272] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.92GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 216/216 - dA[0.0361,0.2972] - dB[0.1007,0.1621] - g[98.8620,99.1547]           >Saved: ./Models_CycleGAN_3/g_model_AtoB_216.h5 and ./Models_CycleGAN_3/g_model_BtoA_216.h5 \n",
      "\n",
      "\n",
      " Epoch 2/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0311,0.1339] - dB[0.3947,0.1186] - g[85.5386,83.0936]         >Saved: ./Models_CycleGAN_3/g_model_AtoB_432.h5 and ./Models_CycleGAN_3/g_model_BtoA_432.h5 \n",
      "\n",
      "\n",
      " Epoch 3/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0101,0.0198] - dB[0.0294,0.0360] - g[72.4350,72.3601]         >Saved: ./Models_CycleGAN_3/g_model_AtoB_648.h5 and ./Models_CycleGAN_3/g_model_BtoA_648.h5 \n",
      "\n",
      "\n",
      " Epoch 4/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0150,0.0467] - dB[0.1608,0.1628] - g[62.2404,62.5604]         >Saved: ./Models_CycleGAN_3/g_model_AtoB_864.h5 and ./Models_CycleGAN_3/g_model_BtoA_864.h5 \n",
      "\n",
      "\n",
      " Epoch 5/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0187,0.0256] - dB[0.0120,0.0193] - g[56.3531,54.4520]         >Saved: ./Models_CycleGAN_3/g_model_AtoB_1080.h5 and ./Models_CycleGAN_3/g_model_BtoA_1080.h5 \n",
      "\n",
      "\n",
      " Epoch 6/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0117,0.0141] - dB[0.1722,0.2098] - g[42.3359,42.4457]         >Saved: ./Models_CycleGAN_3/g_model_AtoB_1512.h5 and ./Models_CycleGAN_3/g_model_BtoA_1512.h5 \n",
      "\n",
      "\n",
      " Epoch 8/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0057,0.0111] - dB[0.1557,0.0181] - g[36.8526,37.5818]         >Saved: ./Models_CycleGAN_3/g_model_AtoB_1728.h5 and ./Models_CycleGAN_3/g_model_BtoA_1728.h5 \n",
      "\n",
      "\n",
      " Epoch 9/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0105,0.0025] - dB[0.0575,0.0126] - g[34.1789,33.8845]         >Saved: ./Models_CycleGAN_3/g_model_AtoB_1944.h5 and ./Models_CycleGAN_3/g_model_BtoA_1944.h5 \n",
      "\n",
      "\n",
      " Epoch 10/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.2143,0.2028] - dB[0.0890,0.0271] - g[31.1662,29.9922]         >Saved: ./Models_CycleGAN_3/g_model_AtoB_2160.h5 and ./Models_CycleGAN_3/g_model_BtoA_2160.h5 \n",
      "\n",
      "\n",
      " Epoch 11/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0266,0.0238] - dB[0.1322,0.0333] - g[27.1051,27.0469]         >Saved: ./Models_CycleGAN_3/g_model_AtoB_2376.h5 and ./Models_CycleGAN_3/g_model_BtoA_2376.h5 \n",
      "\n",
      "\n",
      " Epoch 12/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0026,0.0090] - dB[0.0433,0.0380] - g[23.6607,24.3800]         >Saved: ./Models_CycleGAN_3/g_model_AtoB_2592.h5 and ./Models_CycleGAN_3/g_model_BtoA_2592.h5 \n",
      "\n",
      "\n",
      " Epoch 13/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0083,0.0149] - dB[0.0893,0.0213] - g[20.8661,21.4648]         >Saved: ./Models_CycleGAN_3/g_model_AtoB_2808.h5 and ./Models_CycleGAN_3/g_model_BtoA_2808.h5 \n",
      "\n",
      "\n",
      " Epoch 14/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0087,0.0095] - dB[0.1471,0.0166] - g[20.0151,20.0129]         >Saved: ./Models_CycleGAN_3/g_model_AtoB_3024.h5 and ./Models_CycleGAN_3/g_model_BtoA_3024.h5 \n",
      "\n",
      "\n",
      " Epoch 15/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0112,0.0065] - dB[0.0084,0.0911] - g[18.8774,18.6959]         >Saved: ./Models_CycleGAN_3/g_model_AtoB_3240.h5 and ./Models_CycleGAN_3/g_model_BtoA_3240.h5 \n",
      "\n",
      "\n",
      " Epoch 16/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0568,0.0374] - dB[0.1015,0.4163] - g[15.8905,16.8583]         >Saved: ./Models_CycleGAN_3/g_model_AtoB_3456.h5 and ./Models_CycleGAN_3/g_model_BtoA_3456.h5 \n",
      "\n",
      "\n",
      " Epoch 17/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0019,0.0045] - dB[0.0719,0.0151] - g[16.0423,17.2218]         >Saved: ./Models_CycleGAN_3/g_model_AtoB_3672.h5 and ./Models_CycleGAN_3/g_model_BtoA_3672.h5 \n",
      "\n",
      "\n",
      " Epoch 18/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0028,0.0050] - dB[0.0241,0.1777] - g[12.7706,14.1337]         >Saved: ./Models_CycleGAN_3/g_model_AtoB_3888.h5 and ./Models_CycleGAN_3/g_model_BtoA_3888.h5 \n",
      "\n",
      "\n",
      " Epoch 19/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0023,0.0087] - dB[0.1415,0.2168] - g[12.5038,12.7636]         >Saved: ./Models_CycleGAN_3/g_model_AtoB_4104.h5 and ./Models_CycleGAN_3/g_model_BtoA_4104.h5 \n",
      "\n",
      "\n",
      " Epoch 20/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0031,0.0043] - dB[0.0144,0.0228] - g[10.4317,10.9976]         >Saved: ./Models_CycleGAN_3/g_model_AtoB_4320.h5 and ./Models_CycleGAN_3/g_model_BtoA_4320.h5 \n",
      "\n",
      "\n",
      " Epoch 21/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0041,0.0037] - dB[0.0082,0.0053] - g[11.7960,10.3571]         >Saved: ./Models_CycleGAN_3/g_model_AtoB_4536.h5 and ./Models_CycleGAN_3/g_model_BtoA_4536.h5 \n",
      "\n",
      "\n",
      " Epoch 22/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0754,0.0361] - dB[0.3114,0.0896] - g[9.4859,9.9358]           >Saved: ./Models_CycleGAN_3/g_model_AtoB_4752.h5 and ./Models_CycleGAN_3/g_model_BtoA_4752.h5 \n",
      "\n",
      "\n",
      " Epoch 23/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0023,0.0026] - dB[0.0085,0.0079] - g[8.5298,9.0139]          >Saved: ./Models_CycleGAN_3/g_model_AtoB_4968.h5 and ./Models_CycleGAN_3/g_model_BtoA_4968.h5 \n",
      "\n",
      "\n",
      " Epoch 24/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0019,0.0023] - dB[0.0836,0.1097] - g[7.4952,8.2312]           >Saved: ./Models_CycleGAN_3/g_model_AtoB_5184.h5 and ./Models_CycleGAN_3/g_model_BtoA_5184.h5 \n",
      "\n",
      "\n",
      " Epoch 25/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0040,0.0031] - dB[0.0093,0.2166] - g[6.8965,7.3520]         >Saved: ./Models_CycleGAN_3/g_model_AtoB_5400.h5 and ./Models_CycleGAN_3/g_model_BtoA_5400.h5 \n",
      "\n",
      "\n",
      " Epoch 26/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0006,0.0015] - dB[0.0972,0.1626] - g[7.3430,7.3078]          >Saved: ./Models_CycleGAN_3/g_model_AtoB_5616.h5 and ./Models_CycleGAN_3/g_model_BtoA_5616.h5 \n",
      "\n",
      "\n",
      " Epoch 27/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0054,0.0062] - dB[0.0117,0.0049] - g[7.3325,7.3103]         >Saved: ./Models_CycleGAN_3/g_model_AtoB_5832.h5 and ./Models_CycleGAN_3/g_model_BtoA_5832.h5 \n",
      "\n",
      "\n",
      " Epoch 28/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.0077,0.0202] - dB[0.0912,0.0758] - g[6.0639,6.9425]          >Saved: ./Models_CycleGAN_3/g_model_AtoB_6048.h5 and ./Models_CycleGAN_3/g_model_BtoA_6048.h5 \n",
      "\n",
      "\n",
      " Epoch 29/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.1097,0.1827] - dB[0.0021,0.0411] - g[6.2729,6.4801]          >Saved: ./Models_CycleGAN_3/g_model_AtoB_6264.h5 and ./Models_CycleGAN_3/g_model_BtoA_6264.h5 \n",
      "\n",
      "\n",
      " Epoch 30/100 \n",
      "\n",
      "Batch: 216/216 - dA[0.2510,0.0900] - dB[0.0176,0.0036] - g[4.8576,6.1718]         >Saved: ./Models_CycleGAN_3/g_model_AtoB_6480.h5 and ./Models_CycleGAN_3/g_model_BtoA_6480.h5 \n",
      "\n",
      "\n",
      " Epoch 31/100 \n",
      "\n",
      "Batch: 158/216 - dA[0.1564,0.0613] - dB[0.0068,0.0357] - g[4.8605,4.7623]          "
     ]
    }
   ],
   "source": [
    "# train models\n",
    "train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, filename, img_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7S4NZZ5bwYtQ",
    "tags": []
   },
   "source": [
    "# Test Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 282,
     "status": "ok",
     "timestamp": 1655719013519,
     "user": {
      "displayName": "Alice Henriques da Rosa",
      "userId": "12243278505695630565"
     },
     "user_tz": -60
    },
    "id": "M78fwNkKwh4S"
   },
   "outputs": [],
   "source": [
    "def padding(image,size):\n",
    "\n",
    "    img_reshape = np.moveaxis(image, -1, 0)\n",
    "    \n",
    "    m = ReflectionPad3d((0,size[2]-image.shape[2],0,size[1]-image.shape[1],0,size[0]-image.shape[0]))\n",
    "    input = torch.tensor(img_reshape, dtype=torch.float)\n",
    "    output = m(input)\n",
    "    pad_img = output.numpy()\n",
    "    pad_img = np.moveaxis(pad_img, 0, -1)\n",
    "    \n",
    "    return pad_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1655719013828,
     "user": {
      "displayName": "Alice Henriques da Rosa",
      "userId": "12243278505695630565"
     },
     "user_tz": -60
    },
    "id": "E_EdHMzzwkEp"
   },
   "outputs": [],
   "source": [
    "def thresholding(patch_prediction):\n",
    "    patch_ind_nuclei = np.argwhere(patch_prediction[:,:,:,0] > 0.5)\n",
    "    patch_ind_golgi = np.argwhere(patch_prediction[:,:,:,1] > 0.5)\n",
    "    patch_ind_int = np.argwhere(patch_prediction[:,:,:,2] > 0.5)\n",
    "\n",
    "    patch_prediction_thr = np.zeros((patch_prediction.shape[0],patch_prediction.shape[1],patch_prediction.shape[2],3))\n",
    "\n",
    "    for i in range(patch_ind_nuclei.shape[0]):\n",
    "        patch_prediction_thr[patch_ind_nuclei[i,0],patch_ind_nuclei[i,1],patch_ind_nuclei[i,2],0]=1 \n",
    "\n",
    "    for j in range(patch_ind_golgi.shape[0]):\n",
    "        patch_prediction_thr[patch_ind_golgi[j,0],patch_ind_golgi[j,1],patch_ind_golgi[j,2],1]=1\n",
    "    \n",
    "    for k in range(patch_ind_int.shape[0]):\n",
    "        patch_prediction_thr[patch_ind_int[k,0],patch_ind_int[k,1],patch_ind_int[k,2],2]=1\n",
    "\n",
    "    return patch_prediction_thr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1655719013829,
     "user": {
      "displayName": "Alice Henriques da Rosa",
      "userId": "12243278505695630565"
     },
     "user_tz": -60
    },
    "id": "kFSYuLBPwmBc"
   },
   "outputs": [],
   "source": [
    "def pred_mask(image, mask_shape):\n",
    "    patch_size = 64\n",
    "    step = 48\n",
    "\n",
    "    pred_mask = np.zeros(((image.shape[0] // step)*step + patch_size,(image.shape[1] // step)*step + patch_size,64,3))\n",
    "    _image = padding(image, ((image.shape[0] // step)*step + patch_size,(image.shape[1] // step)*step + patch_size,64,image.shape[3]))\n",
    "\n",
    "    i = 0\n",
    "    while i + patch_size <= _image.shape[0]:\n",
    "        j = 0\n",
    "        while j + patch_size <= _image.shape[1]:\n",
    "\n",
    "            tst_patch = _image[i:i+patch_size, j:j+patch_size, :, :]\n",
    "            tst_patch = np.array([(tst_patch - 127.5) / 127.5])\n",
    "            preds_tst = my_model.predict(tst_patch)\n",
    "            preds_tst = (preds_tst + 1) / 2.0\n",
    "            pred_patch = preds_tst[0,:,:,:,:]\n",
    "            pred_patch_thr = thresholding(pred_patch)\n",
    "\n",
    "            pred_mask[i:i+patch_size, j:j+patch_size,:,0] = np.array(np.logical_or(pred_mask[i:i+patch_size, j:j+patch_size, :,0], pred_patch_thr[:,:,:,0], dtype = 'float32'))\n",
    "            pred_mask[i:i+patch_size, j:j+patch_size,:,1] = np.array(np.logical_or(pred_mask[i:i+patch_size, j:j+patch_size, :,1], pred_patch_thr[:,:,:,1], dtype = 'float32'))\n",
    "            pred_mask[i:i+patch_size, j:j+patch_size,:,2] = np.array(np.logical_or(pred_mask[i:i+patch_size, j:j+patch_size, :,2], pred_patch_thr[:,:,:,2], dtype = 'float32'))\n",
    "\n",
    "            j += step\n",
    "\n",
    "        i += step\n",
    "\n",
    "    _pred_mask = np.zeros(mask_shape)\n",
    "    _pred_mask = pred_mask[0:mask_shape[0],0:mask_shape[1],0:mask_shape[2],:]\n",
    "\n",
    "    return _pred_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 262,
     "status": "ok",
     "timestamp": 1655719014087,
     "user": {
      "displayName": "Alice Henriques da Rosa",
      "userId": "12243278505695630565"
     },
     "user_tz": -60
    },
    "id": "_Ny5V_2WwoJ6"
   },
   "outputs": [],
   "source": [
    "def dice_coefficient(y_true, y_pred):\n",
    "    smoothing_factor = 1\n",
    "    flat_y_true = K.flatten(y_true)\n",
    "    flat_y_pred = K.flatten(y_pred)\n",
    "    return (2. * K.sum(flat_y_true * flat_y_pred) + smoothing_factor) / (K.sum(flat_y_true) + K.sum(flat_y_pred) + smoothing_factor)\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    flat_y_true = K.flatten(y_true)\n",
    "    flat_y_pred = K.flatten(y_pred)\n",
    "    return(K.sum(flat_y_true * flat_y_pred) / K.sum(flat_y_pred) )\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    flat_y_true = K.flatten(y_true)\n",
    "    flat_y_pred = K.flatten(y_pred)\n",
    "    return(K.sum(flat_y_true * flat_y_pred) / K.sum(flat_y_true) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1655719014089,
     "user": {
      "displayName": "Alice Henriques da Rosa",
      "userId": "12243278505695630565"
     },
     "user_tz": -60
    },
    "id": "JkqIRTBAwp47"
   },
   "outputs": [],
   "source": [
    "def metrics(mask, _pred_mask):\n",
    "  _mask = mask/255.\n",
    "\n",
    "  dice_coef_int = dice_coefficient(_mask[:,:,:,2],_pred_mask[:,:,:,2])\n",
    "  dice_coef_nuclei = dice_coefficient(_mask[:,:,:,1],_pred_mask[:,:,:,1])\n",
    "  dice_coef_golgi = dice_coefficient(_mask[:,:,:,0],_pred_mask[:,:,:,0])\n",
    "  prec_int = precision(_mask[:,:,:,2],_pred_mask[:,:,:,2])\n",
    "  prec_nuclei = precision(_mask[:,:,:,1],_pred_mask[:,:,:,1])\n",
    "  prec_golgi = precision(_mask[:,:,:,0],_pred_mask[:,:,:,0])\n",
    "  recall_int = recall(_mask[:,:,:,2],_pred_mask[:,:,:,2])\n",
    "  recall_nuclei = recall(_mask[:,:,:,1],_pred_mask[:,:,:,1])\n",
    "  recall_golgi = recall(_mask[:,:,:,0],_pred_mask[:,:,:,0])\n",
    "\n",
    "  return [round(dice_coef_nuclei.numpy(),4), round(dice_coef_golgi.numpy(),4),round(dice_coef_int.numpy(),4), \n",
    "          round(prec_nuclei.numpy(),4), round(prec_golgi.numpy(),4),round(prec_int.numpy(),4), round(recall_nuclei.numpy(),4),\n",
    "          round(recall_golgi.numpy(),4), round(recall_int.numpy(),4)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1655719014090,
     "user": {
      "displayName": "Alice Henriques da Rosa",
      "userId": "12243278505695630565"
     },
     "user_tz": -60
    },
    "id": "opglOpuzwsIm"
   },
   "outputs": [],
   "source": [
    "def write_to_excel(wb,sheet_name,metrics, result_dir):\n",
    "\n",
    "  # add_sheet is used to create sheet.\n",
    "  sheet1 = wb.add_sheet(sheet_name)\n",
    "\n",
    "  sheet1.write(0, 0, 'Dice Coeffient Nuclei')\n",
    "  sheet1.write(0, 1, 'Dice Coeffient Golgi')\n",
    "  sheet1.write(0, 2, 'Dice Coeffient Intersection')\n",
    "  sheet1.write(0, 3, 'Precision Nuclei')\n",
    "  sheet1.write(0, 4, 'Precision Golgi')\n",
    "  sheet1.write(0, 5, 'Precision Intersection')\n",
    "  sheet1.write(0, 6, 'Recall Nuclei')\n",
    "  sheet1.write(0, 7, 'Recall Golgi')\n",
    "  sheet1.write(0, 8, 'Recall Intersection')\n",
    "  sheet1.write(1, 0, metrics[0])\n",
    "  sheet1.write(1, 1, metrics[1])\n",
    "  sheet1.write(1, 2, metrics[2])\n",
    "  sheet1.write(1, 3, metrics[3])\n",
    "  sheet1.write(1, 4, metrics[4])\n",
    "  sheet1.write(1, 5, metrics[5])\n",
    "  sheet1.write(1, 6, metrics[6])\n",
    "  sheet1.write(1, 7, metrics[7])\n",
    "  sheet1.write(1, 8, metrics[8])\n",
    "\n",
    "  wb.save(result_dir + '/results_metrics.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1655719014090,
     "user": {
      "displayName": "Alice Henriques da Rosa",
      "userId": "12243278505695630565"
     },
     "user_tz": -60
    },
    "id": "8yDXNAWGwviV"
   },
   "outputs": [],
   "source": [
    "def test_model(base_dir):\n",
    "\n",
    "    fnames = os.listdir(base_dir + 'Images/')\n",
    "\n",
    "    # Workbook is created\n",
    "    wb = Workbook()\n",
    "\n",
    "    result_dir = './Dataset_3/Results_CycleGAN'\n",
    "    if os.path.exists(result_dir)==False:\n",
    "        os.mkdir(result_dir)\n",
    "\n",
    "    for i in range(len(fnames)):\n",
    "        \n",
    "        image = imread(base_dir + 'Images/' + fnames[i])\n",
    "        mask = imread(base_dir + 'Masks/' + fnames[i])\n",
    "\n",
    "        predicted_mask = pred_mask(image, image.shape)\n",
    "\n",
    "        _predicted_mask = predicted_mask*255.0\n",
    "        _predicted_mask = _predicted_mask.astype('uint8')\n",
    "\n",
    "        imwrite(result_dir + '/pred_mask_' + fnames[i] , _predicted_mask, photometric='rgb')\n",
    "\n",
    "        _metrics = metrics(mask, predicted_mask)\n",
    "\n",
    "        write_to_excel(wb,'Sheet_' + fnames[i].split('.')[0], _metrics, result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 136212,
     "status": "ok",
     "timestamp": 1655719184999,
     "user": {
      "displayName": "Alice Henriques da Rosa",
      "userId": "12243278505695630565"
     },
     "user_tz": -60
    },
    "id": "8Uk3-amtwwMW",
    "outputId": "635ef0c8-f8e9-46db-a347-cbf291a61844"
   },
   "outputs": [],
   "source": [
    "#Load the pretrained model for testing and predictions. \n",
    "\n",
    "#Load the pretrained model for testing and predictions. \n",
    "from keras.models import load_model\n",
    "my_model = load_model('./Models_CycleGAN_3/g_model_AtoB_8208.h5', compile=False, custom_objects={'InstanceNormalization':InstanceNormalization})\n",
    "#If you load a different model do not forget to preprocess accordingly. \n",
    "\n",
    "base_dir = './Dataset_3/Patches_synthetic_64/Test/'\n",
    "\n",
    "test_model(base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgOh29oJMzW3",
    "tags": []
   },
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(filename, dataset):\n",
    "    \n",
    "    list_IDs_mask = os.listdir(filename + dataset)\n",
    "    list_IDs_images = os.listdir(filename + '/Images/')\n",
    "    i = 0\n",
    "    j = 0\n",
    "\n",
    "    for ID in list_IDs_mask:\n",
    "        X = imread(filename + dataset + ID)[:,:,:,0]\n",
    "        \n",
    "        is_all_zeros = np.all((X == 0))\n",
    "        \n",
    "        if is_all_zeros:\n",
    "            #os.remove(filename + dataset + ID)\n",
    "            print(\"File\",ID,\"deleted mask\")\n",
    "            j = j+1\n",
    "            \n",
    "    for ID in list_IDs_images:\n",
    "        y = imread(filename + '/Images/' + ID)[:,:,:,0]\n",
    "        is_all_low = np.all((y <= (200/255.)))\n",
    "            \n",
    "        if is_all_low:\n",
    "            #os.remove(filename + '/Images/' + ID)\n",
    "            print(\"File\",ID,\"deleted image\")\n",
    "            i = i+1\n",
    "            \n",
    "    print(len(list_IDs_mask), j)\n",
    "    print(len(list_IDs_images), i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File img_patch_644.tif deleted mask\n",
      "File img_patch_124.tif deleted mask\n",
      "File img_patch_564.tif deleted mask\n",
      "File img_patch_518.tif deleted mask\n",
      "File img_patch_438.tif deleted mask\n",
      "File img_patch_252.tif deleted mask\n",
      "File img_patch_161.tif deleted mask\n",
      "File img_patch_563.tif deleted mask\n",
      "File img_patch_588.tif deleted mask\n",
      "File img_patch_551.tif deleted mask\n",
      "File img_patch_321.tif deleted mask\n",
      "File img_patch_50.tif deleted mask\n",
      "File img_patch_145.tif deleted mask\n",
      "File img_patch_437.tif deleted mask\n",
      "File img_patch_528.tif deleted mask\n",
      "File img_patch_547.tif deleted mask\n",
      "File img_patch_128.tif deleted mask\n",
      "File img_patch_443.tif deleted mask\n",
      "File img_patch_444.tif deleted mask\n",
      "File img_patch_34.tif deleted mask\n",
      "File img_patch_375.tif deleted mask\n",
      "File img_patch_118.tif deleted mask\n",
      "File img_patch_225.tif deleted mask\n",
      "File img_patch_31.tif deleted mask\n",
      "File img_patch_631.tif deleted mask\n",
      "File img_patch_338.tif deleted mask\n",
      "File img_patch_32.tif deleted mask\n",
      "File img_patch_647.tif deleted mask\n",
      "File img_patch_670.tif deleted mask\n",
      "File img_patch_147.tif deleted mask\n",
      "File img_patch_584.tif deleted mask\n",
      "File img_patch_513.tif deleted mask\n",
      "File img_patch_333.tif deleted mask\n",
      "File img_patch_540.tif deleted mask\n",
      "File img_patch_273.tif deleted mask\n",
      "File img_patch_15.tif deleted mask\n",
      "File img_patch_332.tif deleted mask\n",
      "File img_patch_327.tif deleted mask\n",
      "File img_patch_662.tif deleted mask\n",
      "File img_patch_11.tif deleted mask\n",
      "File img_patch_138.tif deleted mask\n",
      "File img_patch_326.tif deleted mask\n",
      "File img_patch_260.tif deleted mask\n",
      "File img_patch_523.tif deleted mask\n",
      "File img_patch_669.tif deleted mask\n",
      "File img_patch_233.tif deleted mask\n",
      "File img_patch_271.tif deleted mask\n",
      "File img_patch_524.tif deleted mask\n",
      "File img_patch_526.tif deleted mask\n",
      "File img_patch_20.tif deleted mask\n",
      "File img_patch_129.tif deleted mask\n",
      "File img_patch_60.tif deleted mask\n",
      "File img_patch_112.tif deleted mask\n",
      "File img_patch_565.tif deleted mask\n",
      "File img_patch_552.tif deleted mask\n",
      "File img_patch_231.tif deleted mask\n",
      "File img_patch_560.tif deleted mask\n",
      "File img_patch_211.tif deleted mask\n",
      "File img_patch_430.tif deleted mask\n",
      "File img_patch_133.tif deleted mask\n",
      "File img_patch_39.tif deleted mask\n",
      "File img_patch_269.tif deleted mask\n",
      "File img_patch_367.tif deleted mask\n",
      "File img_patch_370.tif deleted mask\n",
      "File img_patch_555.tif deleted mask\n",
      "File img_patch_521.tif deleted mask\n",
      "File img_patch_531.tif deleted mask\n",
      "File img_patch_10.tif deleted mask\n",
      "File img_patch_240.tif deleted mask\n",
      "File img_patch_137.tif deleted mask\n",
      "File img_patch_162.tif deleted mask\n",
      "File img_patch_341.tif deleted mask\n",
      "File img_patch_242.tif deleted mask\n",
      "File img_patch_553.tif deleted mask\n",
      "File img_patch_581.tif deleted mask\n",
      "File img_patch_217.tif deleted mask\n",
      "File img_patch_661.tif deleted mask\n",
      "File img_patch_512.tif deleted mask\n",
      "File img_patch_43.tif deleted mask\n",
      "File img_patch_427.tif deleted mask\n",
      "File img_patch_239.tif deleted mask\n",
      "File img_patch_533.tif deleted mask\n",
      "File img_patch_546.tif deleted mask\n",
      "File img_patch_611.tif deleted mask\n",
      "File img_patch_363.tif deleted mask\n",
      "File img_patch_352.tif deleted mask\n",
      "File img_patch_210.tif deleted mask\n",
      "File img_patch_535.tif deleted mask\n",
      "File img_patch_425.tif deleted mask\n",
      "File img_patch_525.tif deleted mask\n",
      "File img_patch_155.tif deleted mask\n",
      "File img_patch_156.tif deleted mask\n",
      "File img_patch_215.tif deleted mask\n",
      "File img_patch_376.tif deleted mask\n",
      "File img_patch_658.tif deleted mask\n",
      "File img_patch_55.tif deleted mask\n",
      "File img_patch_69.tif deleted mask\n",
      "File img_patch_268.tif deleted mask\n",
      "File img_patch_276.tif deleted mask\n",
      "File img_patch_436.tif deleted mask\n",
      "File img_patch_374.tif deleted mask\n",
      "File img_patch_573.tif deleted mask\n",
      "File img_patch_514.tif deleted mask\n",
      "File img_patch_234.tif deleted mask\n",
      "File img_patch_440.tif deleted mask\n",
      "File img_patch_515.tif deleted mask\n",
      "File img_patch_146.tif deleted mask\n",
      "File img_patch_135.tif deleted mask\n",
      "File img_patch_572.tif deleted mask\n",
      "File img_patch_632.tif deleted mask\n",
      "File img_patch_35.tif deleted mask\n",
      "File img_patch_435.tif deleted mask\n",
      "File img_patch_262.tif deleted mask\n",
      "File img_patch_127.tif deleted mask\n",
      "File img_patch_319.tif deleted mask\n",
      "File img_patch_586.tif deleted mask\n",
      "File img_patch_261.tif deleted mask\n",
      "File img_patch_645.tif deleted mask\n",
      "File img_patch_534.tif deleted mask\n",
      "File img_patch_218.tif deleted mask\n",
      "File img_patch_322.tif deleted mask\n",
      "File img_patch_580.tif deleted mask\n",
      "File img_patch_316.tif deleted mask\n",
      "File img_patch_310.tif deleted mask\n",
      "File img_patch_114.tif deleted mask\n",
      "File img_patch_585.tif deleted mask\n",
      "File img_patch_373.tif deleted mask\n",
      "File img_patch_612.tif deleted mask\n",
      "File img_patch_312.tif deleted mask\n",
      "File img_patch_272.tif deleted mask\n",
      "File img_patch_558.tif deleted mask\n",
      "File img_patch_24.tif deleted mask\n",
      "File img_patch_144.tif deleted mask\n",
      "File img_patch_652.tif deleted mask\n",
      "File img_patch_544.tif deleted mask\n",
      "File img_patch_621.tif deleted mask\n",
      "File img_patch_559.tif deleted mask\n",
      "File img_patch_571.tif deleted mask\n",
      "File img_patch_556.tif deleted mask\n",
      "File img_patch_25.tif deleted mask\n",
      "File img_patch_246.tif deleted mask\n",
      "File img_patch_362.tif deleted mask\n",
      "File img_patch_213.tif deleted mask\n",
      "File img_patch_371.tif deleted mask\n",
      "File img_patch_629.tif deleted mask\n",
      "File img_patch_221.tif deleted mask\n",
      "File img_patch_235.tif deleted mask\n",
      "File img_patch_575.tif deleted mask\n",
      "File img_patch_131.tif deleted mask\n",
      "File img_patch_522.tif deleted mask\n",
      "File img_patch_537.tif deleted mask\n",
      "File img_patch_539.tif deleted mask\n",
      "File img_patch_532.tif deleted mask\n",
      "File img_patch_238.tif deleted mask\n",
      "File img_patch_663.tif deleted mask\n",
      "File img_patch_620.tif deleted mask\n",
      "File img_patch_58.tif deleted mask\n",
      "File img_patch_369.tif deleted mask\n",
      "File img_patch_22.tif deleted mask\n",
      "File img_patch_263.tif deleted mask\n",
      "File img_patch_232.tif deleted mask\n",
      "File img_patch_265.tif deleted mask\n",
      "File img_patch_665.tif deleted mask\n",
      "File img_patch_115.tif deleted image\n",
      "File img_patch_564.tif deleted image\n",
      "File img_patch_518.tif deleted image\n",
      "File img_patch_577.tif deleted image\n",
      "File img_patch_50.tif deleted image\n",
      "File img_patch_227.tif deleted image\n",
      "File img_patch_582.tif deleted image\n",
      "File img_patch_520.tif deleted image\n",
      "File img_patch_68.tif deleted image\n",
      "File img_patch_528.tif deleted image\n",
      "File img_patch_547.tif deleted image\n",
      "File img_patch_223.tif deleted image\n",
      "File img_patch_650.tif deleted image\n",
      "File img_patch_27.tif deleted image\n",
      "File img_patch_447.tif deleted image\n",
      "File img_patch_610.tif deleted image\n",
      "File img_patch_519.tif deleted image\n",
      "File img_patch_225.tif deleted image\n",
      "File img_patch_249.tif deleted image\n",
      "File img_patch_562.tif deleted image\n",
      "File img_patch_631.tif deleted image\n",
      "File img_patch_584.tif deleted image\n",
      "File img_patch_258.tif deleted image\n",
      "File img_patch_637.tif deleted image\n",
      "File img_patch_643.tif deleted image\n",
      "File img_patch_378.tif deleted image\n",
      "File img_patch_567.tif deleted image\n",
      "File img_patch_540.tif deleted image\n",
      "File img_patch_273.tif deleted image\n",
      "File img_patch_566.tif deleted image\n",
      "File img_patch_432.tif deleted image\n",
      "File img_patch_324.tif deleted image\n",
      "File img_patch_431.tif deleted image\n",
      "File img_patch_278.tif deleted image\n",
      "File img_patch_526.tif deleted image\n",
      "File img_patch_18.tif deleted image\n",
      "File img_patch_619.tif deleted image\n",
      "File img_patch_23.tif deleted image\n",
      "File img_patch_60.tif deleted image\n",
      "File img_patch_641.tif deleted image\n",
      "File img_patch_328.tif deleted image\n",
      "File img_patch_552.tif deleted image\n",
      "File img_patch_339.tif deleted image\n",
      "File img_patch_538.tif deleted image\n",
      "File img_patch_211.tif deleted image\n",
      "File img_patch_159.tif deleted image\n",
      "File img_patch_63.tif deleted image\n",
      "File img_patch_59.tif deleted image\n",
      "File img_patch_67.tif deleted image\n",
      "File img_patch_411.tif deleted image\n",
      "File img_patch_417.tif deleted image\n",
      "File img_patch_615.tif deleted image\n",
      "File img_patch_140.tif deleted image\n",
      "File img_patch_222.tif deleted image\n",
      "File img_patch_377.tif deleted image\n",
      "File img_patch_126.tif deleted image\n",
      "File img_patch_51.tif deleted image\n",
      "File img_patch_441.tif deleted image\n",
      "File img_patch_43.tif deleted image\n",
      "File img_patch_427.tif deleted image\n",
      "File img_patch_318.tif deleted image\n",
      "File img_patch_323.tif deleted image\n",
      "File img_patch_546.tif deleted image\n",
      "File img_patch_611.tif deleted image\n",
      "File img_patch_54.tif deleted image\n",
      "File img_patch_589.tif deleted image\n",
      "File img_patch_16.tif deleted image\n",
      "File img_patch_425.tif deleted image\n",
      "File img_patch_313.tif deleted image\n",
      "File img_patch_154.tif deleted image\n",
      "File img_patch_376.tif deleted image\n",
      "File img_patch_116.tif deleted image\n",
      "File img_patch_255.tif deleted image\n",
      "File img_patch_55.tif deleted image\n",
      "File img_patch_117.tif deleted image\n",
      "File img_patch_276.tif deleted image\n",
      "File img_patch_436.tif deleted image\n",
      "File img_patch_125.tif deleted image\n",
      "File img_patch_630.tif deleted image\n",
      "File img_patch_311.tif deleted image\n",
      "File img_patch_529.tif deleted image\n",
      "File img_patch_135.tif deleted image\n",
      "File img_patch_35.tif deleted image\n",
      "File img_patch_435.tif deleted image\n",
      "File img_patch_277.tif deleted image\n",
      "File img_patch_19.tif deleted image\n",
      "File img_patch_46.tif deleted image\n",
      "File img_patch_47.tif deleted image\n",
      "File img_patch_654.tif deleted image\n",
      "File img_patch_617.tif deleted image\n",
      "File img_patch_379.tif deleted image\n",
      "File img_patch_580.tif deleted image\n",
      "File img_patch_541.tif deleted image\n",
      "File img_patch_612.tif deleted image\n",
      "File img_patch_312.tif deleted image\n",
      "File img_patch_272.tif deleted image\n",
      "File img_patch_558.tif deleted image\n",
      "File img_patch_642.tif deleted image\n",
      "File img_patch_583.tif deleted image\n",
      "File img_patch_434.tif deleted image\n",
      "File img_patch_372.tif deleted image\n",
      "File img_patch_421.tif deleted image\n",
      "File img_patch_559.tif deleted image\n",
      "File img_patch_25.tif deleted image\n",
      "File img_patch_246.tif deleted image\n",
      "File img_patch_64.tif deleted image\n",
      "File img_patch_542.tif deleted image\n",
      "File img_patch_362.tif deleted image\n",
      "File img_patch_419.tif deleted image\n",
      "File img_patch_213.tif deleted image\n",
      "File img_patch_587.tif deleted image\n",
      "File img_patch_537.tif deleted image\n",
      "File img_patch_539.tif deleted image\n",
      "File img_patch_329.tif deleted image\n",
      "File img_patch_532.tif deleted image\n",
      "File img_patch_111.tif deleted image\n",
      "File img_patch_17.tif deleted image\n",
      "File img_patch_511.tif deleted image\n",
      "File img_patch_649.tif deleted image\n",
      "File img_patch_245.tif deleted image\n",
      "File img_patch_58.tif deleted image\n",
      "File img_patch_579.tif deleted image\n",
      "File img_patch_369.tif deleted image\n",
      "File img_patch_37.tif deleted image\n",
      "File img_patch_22.tif deleted image\n",
      "File img_patch_26.tif deleted image\n",
      "File img_patch_232.tif deleted image\n",
      "File img_patch_568.tif deleted image\n",
      "File img_patch_651.tif deleted image\n",
      "File img_patch_344.tif deleted image\n",
      "File img_patch_665.tif deleted image\n",
      "433 163\n",
      "433 131\n",
      "433\n",
      "433\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "pre_processing('./Dataset_3/Patches_synthetic_64/Train', '/Masks/')\n",
    "list_IDs = os.listdir('./Dataset_3/Patches_synthetic_64/Train' + '/Masks/')\n",
    "print(len(list_IDs))\n",
    "list_IDs_ = os.listdir('./Dataset_3/Patches_synthetic_64/Train' + '/Images/')\n",
    "print(len(list_IDs_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f94edb7c310>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = imread('./Dataset/Patches_synthetic/Train' + '/Images/' + 'img_patch_110.tif')\n",
    "plt.figure(1)\n",
    "imshow(a[:,:,30,:])\n",
    "c = imread('./Dataset/Patches_synthetic/Train' + '/Masks/' + 'img_patch_110.tif')\n",
    "b = np.zeros((64,64,64,3))\n",
    "b[:,:,:,:2] = c\n",
    "plt.figure(2)\n",
    "imshow(b[:,:,30,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_patches = 37\n",
    "\n",
    "for i in range(n_patches):\n",
    "    patch = np.zeros((64,64,64,2), dtype='float32')\n",
    "    imwrite('./Dataset/Patches_synthetic/Train/Masks' + '/img_patch_' + str(7) + str(i) + '.tif', patch)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNsBO+gt9lsQYL2a5E+6P0i",
   "collapsed_sections": [
    "fgOh29oJMzW3"
   ],
   "name": "CycleGAN_V3.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
