{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Ib54cjC0nmf"
   },
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3993,
     "status": "ok",
     "timestamp": 1658145404120,
     "user": {
      "displayName": "Alice Rosa",
      "userId": "08910053332263565279"
     },
     "user_tz": -60
    },
    "id": "shVYKQ8k0iUo"
   },
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tifffile import imread, imsave, imwrite\n",
    "import math\n",
    "import os, shutil\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LeakyReLU, Conv3D, ZeroPadding3D, MaxPooling3D, UpSampling3D, concatenate, Conv3DTranspose, BatchNormalization, Dropout, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Activation, MaxPool2D, Concatenate\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tifffile import imread\n",
    "import math\n",
    "import xlwt\n",
    "from sys import stdout\n",
    "from xlwt import Workbook\n",
    "from keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\n",
    "from torch.nn import ReflectionPad3d\n",
    "import torch\n",
    "from matplotlib.pyplot import imshow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCsfODtO0q2t",
    "tags": []
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1658145404120,
     "user": {
      "displayName": "Alice Rosa",
      "userId": "08910053332263565279"
     },
     "user_tz": -60
    },
    "id": "wtWqP5po0u4u"
   },
   "outputs": [],
   "source": [
    "def Generator(img_shape):\n",
    "    '''\n",
    "    Generator model\n",
    "    '''\n",
    "    def encoder_step(layer, Nf, ks, norm=True):\n",
    "        x = Conv3D(Nf, kernel_size=ks, strides=2, kernel_initializer='he_normal', padding='same')(layer)\n",
    "        if norm:\n",
    "            x = InstanceNormalization()(x)\n",
    "        x = LeakyReLU()(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def bottlenek(layer, Nf, ks):\n",
    "        x = Conv3D(Nf, kernel_size=ks, strides=2, kernel_initializer='he_normal', padding='same')(layer)\n",
    "        x = InstanceNormalization()(x)\n",
    "        x = LeakyReLU()(x)\n",
    "        for i in range(4):\n",
    "            y = Conv3D(Nf, kernel_size=ks, strides=1, kernel_initializer='he_normal', padding='same')(x)\n",
    "            x = InstanceNormalization()(y)\n",
    "            x = LeakyReLU()(x)\n",
    "            x = Concatenate()([x, y])\n",
    "\n",
    "        return x\n",
    "\n",
    "    def decoder_step(layer, layer_to_concatenate, Nf, ks):\n",
    "        x = Conv3DTranspose(Nf, kernel_size=ks, strides=2, padding='same', kernel_initializer='he_normal')(layer)\n",
    "        x = InstanceNormalization()(x)\n",
    "        x = LeakyReLU()(x)\n",
    "        x = Concatenate()([x, layer_to_concatenate])\n",
    "        x = Dropout(0.2)(x)\n",
    "        return x\n",
    "\n",
    "    layers_to_concatenate = []\n",
    "    inputs = Input((64,64,64,3), name='input_image')\n",
    "    Nfilter_start = 64\n",
    "    depth = 4\n",
    "    ks = 4\n",
    "    x = inputs\n",
    "\n",
    "    # encoder\n",
    "    for d in range(depth-1):\n",
    "        if d==0:\n",
    "            x = encoder_step(x, Nfilter_start*np.power(2,d), ks, False)\n",
    "        else:\n",
    "            x = encoder_step(x, Nfilter_start*np.power(2,d), ks)\n",
    "        layers_to_concatenate.append(x)\n",
    "\n",
    "    # bottlenek\n",
    "    x = bottlenek(x, Nfilter_start*np.power(2,depth-1), ks)\n",
    "\n",
    "    # decoder\n",
    "    for d in range(depth-2, -1, -1): \n",
    "        x = decoder_step(x, layers_to_concatenate.pop(), Nfilter_start*np.power(2,d), ks)\n",
    "\n",
    "    # classifier\n",
    "    last = Conv3DTranspose(3, kernel_size=ks, strides=2, padding='same', kernel_initializer='he_normal', activation='sigmoid', name='output_generator')(x)\n",
    "   \n",
    "    return Model(inputs=inputs, outputs=last, name='Generator')\n",
    "\n",
    "def Discriminator(img_shape, mask_shape):\n",
    "    '''\n",
    "    Discriminator model\n",
    "    '''\n",
    "\n",
    "    inputs = Input(img_shape, name='input_image')\n",
    "    targets = Input(mask_shape, name='target_image')\n",
    "    Nfilter_start = 64\n",
    "    depth = 3\n",
    "    ks = 4\n",
    "\n",
    "    def encoder_step(layer, Nf, norm=True):\n",
    "        x = Conv3D(Nf, kernel_size=ks, strides=2, kernel_initializer='he_normal', padding='same')(layer)\n",
    "        if norm:\n",
    "            x = InstanceNormalization()(x)\n",
    "        x = LeakyReLU()(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    x = Concatenate()([inputs, targets])\n",
    "\n",
    "    for d in range(depth):\n",
    "        if d==0:\n",
    "            x = encoder_step(x, Nfilter_start*np.power(2,d), False)\n",
    "        else:\n",
    "            x = encoder_step(x, Nfilter_start*np.power(2,d))\n",
    "            \n",
    "    x = ZeroPadding3D()(x)\n",
    "    x = Conv3D(Nfilter_start*(2**depth), ks, strides=1, padding='valid', kernel_initializer='he_normal')(x) \n",
    "    x = InstanceNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "      \n",
    "    x = ZeroPadding3D()(x)\n",
    "    last = Conv3D(1, ks, strides=1, padding='valid', kernel_initializer='he_normal', name='output_discriminator')(x) \n",
    "\n",
    "    return Model(inputs=[inputs, targets], outputs=last, name='Discriminator')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUvrmDb638W-",
    "tags": []
   },
   "source": [
    "# Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1658145404121,
     "user": {
      "displayName": "Alice Rosa",
      "userId": "08910053332263565279"
     },
     "user_tz": -60
    },
    "id": "k9M_RzsQ3_rM"
   },
   "outputs": [],
   "source": [
    "\n",
    "def diceLoss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.convert_to_tensor(y_pred, y_true.dtype)\n",
    "\n",
    "    num = tf.math.reduce_sum(tf.math.multiply(y_true, y_pred))\n",
    "    den = tf.math.reduce_sum(tf.math.add(y_true, y_pred))+1e-5\n",
    "\n",
    "    return 1-2*num/den\n",
    "\n",
    "def weighted_diceLoss(y_true,y_pred, class_weight=[6.63,1,1]):\n",
    "    \n",
    "    weighted_loss = class_weight[0]*diceLoss(y_true[:,:,:,:,0], y_pred[:,:,:,:,0])+class_weight[1]*diceLoss(y_true[:,:,:,:,1], y_pred[:,:,:,:,1])+class_weight[2]*diceLoss(y_true[:,:,:,:,2], y_pred[:,:,:,:,2])\n",
    "    loss = weighted_loss/np.sum(class_weight)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def discriminator_loss(disc_real_output, disc_fake_output):\n",
    "    real_loss = tf.math.reduce_mean(tf.math.pow(tf.ones_like(disc_real_output) - disc_real_output, 2))\n",
    "    fake_loss = tf.math.reduce_mean(tf.math.pow(tf.zeros_like(disc_fake_output) - disc_fake_output, 2))\n",
    "\n",
    "    disc_loss = 0.5*(real_loss + fake_loss)\n",
    "\n",
    "    return disc_loss\n",
    "\n",
    "\n",
    "def generator_loss(target, gen_output, disc_fake_output, alpha):\n",
    "    \n",
    "    # generalized dice loss\n",
    "    dice_loss = weighted_diceLoss(target, gen_output)\n",
    "    \n",
    "    # disc loss\n",
    "    disc_loss = tf.math.reduce_mean(tf.math.pow(tf.ones_like(disc_fake_output) - disc_fake_output, 2))\n",
    "       \n",
    "    # total loss\n",
    "    gen_loss = alpha*dice_loss + disc_loss\n",
    "\n",
    "    return gen_loss, dice_loss, disc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvR5EPUL4RMs",
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 2607,
     "status": "ok",
     "timestamp": 1658145406719,
     "user": {
      "displayName": "Alice Rosa",
      "userId": "08910053332263565279"
     },
     "user_tz": -60
    },
    "id": "Pdal82yN4Tsx"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-15 13:27:26.674072: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-15 13:27:26.681281: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-15 13:27:26.681604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-15 13:27:26.682408: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-15 13:27:26.682744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-15 13:27:26.683028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-15 13:27:26.683184: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-15 13:27:27.068345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-15 13:27:27.068518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-15 13:27:27.068652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-15 13:27:27.068791: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7101 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "# Models \n",
    "img_shape = (64,64,64,3)\n",
    "mask_shape = (64,64,64,3)\n",
    "\n",
    "G = Generator(img_shape)\n",
    "D = Discriminator(img_shape, mask_shape)\n",
    "\n",
    "# Optimizers\n",
    "generator_optimizer = tf.keras.optimizers.Adam(2e-5, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(2e-5, beta_1=0.5)\n",
    "\n",
    "\n",
    "def train_step(image, target, alpha):\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "\n",
    "            gen_output = G(image, training=True)\n",
    "\n",
    "            disc_real_output = D([image, target], training=True)\n",
    "            disc_fake_output = D([image, gen_output], training=True)\n",
    "            disc_loss = discriminator_loss(disc_real_output, disc_fake_output)\n",
    "            \n",
    "            gen_loss, dice_loss, disc_loss_gen = generator_loss(target, gen_output, disc_fake_output, alpha)\n",
    "\n",
    "    generator_gradients = gen_tape.gradient(gen_loss, G.trainable_variables)\n",
    "    discriminator_gradients = disc_tape.gradient(disc_loss, D.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(generator_gradients, G.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(discriminator_gradients, D.trainable_variables))\n",
    "        \n",
    "    return gen_loss, dice_loss, disc_loss_gen\n",
    "\n",
    "\n",
    "def test_step(image, target, alpha):\n",
    "    gen_output = G(image, training=False)\n",
    "\n",
    "    disc_real_output = D([image, target], training=False)\n",
    "    disc_fake_output = D([image, gen_output], training=False)\n",
    "    disc_loss = discriminator_loss(disc_real_output, disc_fake_output)\n",
    "\n",
    "    gen_loss, dice_loss, disc_loss_gen = generator_loss(target, gen_output, disc_fake_output, alpha)\n",
    "        \n",
    "    return gen_loss, dice_loss, disc_loss_gen\n",
    "\n",
    "\n",
    "def thresholding(patch_prediction):\n",
    "    patch_ind_nuclei = np.argwhere(patch_prediction[:,:,:,0] > 0.5)\n",
    "    patch_ind_golgi = np.argwhere(patch_prediction[:,:,:,1] > 0.5)\n",
    "\n",
    "    patch_prediction_thr = np.zeros((patch_prediction.shape[0],patch_prediction.shape[1],patch_prediction.shape[2],3))\n",
    "\n",
    "    for i in range(patch_ind_nuclei.shape[0]):\n",
    "      patch_prediction_thr[patch_ind_nuclei[i,0],patch_ind_nuclei[i,1],patch_ind_nuclei[i,2],0]=1 \n",
    "\n",
    "    for j in range(patch_ind_golgi.shape[0]):\n",
    "      patch_prediction_thr[patch_ind_golgi[j,0],patch_ind_golgi[j,1],patch_ind_golgi[j,2],1]=1\n",
    "\n",
    "    return patch_prediction_thr \n",
    "\n",
    "\n",
    "def fit(train_gen, valid_gen, alpha, epochs):\n",
    "\n",
    "  path = './Dataset_3/Results_Pix2Pix'\n",
    "  if os.path.exists(path)==False:\n",
    "    os.mkdir(path)\n",
    "\n",
    "  \n",
    "  Nt = len(train_gen)\n",
    "  history = {'train': [], 'valid': []}\n",
    "  prev_loss = np.inf\n",
    "  n = 0\n",
    "    \n",
    "  epoch_v2v_loss = tf.keras.metrics.Mean()\n",
    "  epoch_dice_loss = tf.keras.metrics.Mean()\n",
    "  epoch_disc_loss = tf.keras.metrics.Mean()\n",
    "  epoch_v2v_loss_val = tf.keras.metrics.Mean()\n",
    "  epoch_dice_loss_val = tf.keras.metrics.Mean()\n",
    "  epoch_disc_loss_val = tf.keras.metrics.Mean()\n",
    "\n",
    "  for e in range(epochs):\n",
    "    print('Epoch {}/{}'.format(e+1,epochs))\n",
    "    b = 0\n",
    "    \n",
    "    for Xb, yb in train_gen:\n",
    "        b += 1\n",
    "\n",
    "        losses = train_step(Xb, yb, alpha)\n",
    "        epoch_v2v_loss.update_state(losses[0])\n",
    "        epoch_dice_loss.update_state(losses[1])\n",
    "        epoch_disc_loss.update_state(losses[2])\n",
    "        \n",
    "        stdout.write('\\rBatch: {}/{} - loss: {:.4f} - dice_loss: {:.4f} - disc_loss: {:.4f}'\n",
    "                      .format(b, Nt, epoch_v2v_loss.result(), epoch_dice_loss.result(), epoch_disc_loss.result()))\n",
    "        stdout.flush()\n",
    "    history['train'].append([epoch_v2v_loss.result(), epoch_dice_loss.result(), epoch_disc_loss.result()])\n",
    "    \n",
    "    for Xb, yb in valid_gen:\n",
    "        losses_val = test_step(Xb, yb, alpha)\n",
    "        epoch_v2v_loss_val.update_state(losses_val[0])\n",
    "        epoch_dice_loss_val.update_state(losses_val[1])\n",
    "        epoch_disc_loss_val.update_state(losses_val[2])\n",
    "            \n",
    "    stdout.write('\\n               loss_val: {:.4f} - dice_loss_val: {:.4f} - disc_loss_val: {:.4f}'\n",
    "                  .format(epoch_v2v_loss_val.result(), epoch_dice_loss_val.result(), epoch_disc_loss_val.result()))\n",
    "    stdout.flush()\n",
    "    history['valid'].append([epoch_v2v_loss_val.result(), epoch_dice_loss_val.result(), epoch_disc_loss_val.result()])\n",
    "    \n",
    "    # save pred image at epoch e\n",
    "    #y_pred = G.predict(Xb)\n",
    "    #pred_patch = y_pred[0,:,:,:,:]\n",
    "    #pred_patch_thr = thresholding(pred_patch)\n",
    "\n",
    "    #predicted_mask = pred_patch_thr*255.0\n",
    "    #predicted_mask = predicted_mask.astype('uint8')\n",
    "\n",
    "    #imwrite(path + '/pred_mask_' + str(e+1) , predicted_mask, photometric='rgb')\n",
    "\n",
    "    # save models \n",
    "    path_models = './Models_Pix2Pix'\n",
    "    if os.path.exists(path_models)==False:\n",
    "      os.mkdir(path_models)\n",
    "\n",
    "    print(' ')\n",
    "    if epoch_v2v_loss_val.result() < prev_loss:    \n",
    "        G.save_weights(path_models + '/Generator_3channel.h5') \n",
    "        D.save_weights(path_models + '/Discriminator_3channel.h5')\n",
    "        print(\"Validation loss decresaed from {:.4f} to {:.4f}. Models' weights are now saved.\".format(prev_loss, epoch_v2v_loss_val.result()))\n",
    "        prev_loss = epoch_v2v_loss_val.result()\n",
    "        n = 0\n",
    "    else:\n",
    "        print(\"Validation loss did not decrese from {:.4f}.\".format(prev_loss))\n",
    "        # Early Stopping\n",
    "        n = n+1\n",
    "        if n > 20:\n",
    "            return history\n",
    "    print(' ')\n",
    "\n",
    "    # resets losses states\n",
    "    epoch_v2v_loss.reset_states()\n",
    "    epoch_dice_loss.reset_states()\n",
    "    epoch_disc_loss.reset_states()\n",
    "    epoch_v2v_loss_val.reset_states()\n",
    "    epoch_dice_loss_val.reset_states()\n",
    "    epoch_disc_loss_val.reset_states()\n",
    "    \n",
    "    del Xb, yb\n",
    "        \n",
    "  return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVFlG5TVDlYy",
    "tags": []
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 242,
     "status": "ok",
     "timestamp": 1658145444537,
     "user": {
      "displayName": "Alice Rosa",
      "userId": "08910053332263565279"
     },
     "user_tz": -60
    },
    "id": "SJqkDbvHTLWs"
   },
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.insert(0, './')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1658145445411,
     "user": {
      "displayName": "Alice Rosa",
      "userId": "08910053332263565279"
     },
     "user_tz": -60
    },
    "id": "xKjwBmuCEGD_"
   },
   "outputs": [],
   "source": [
    "import data_generator_3\n",
    "from data_generator_3 import DataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "npm90hJQDk_Y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-15 13:27:29.318017: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8101\n",
      "2022-10-15 13:27:29.472716: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-10-15 13:27:29.473063: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-10-15 13:27:29.473085: W tensorflow/stream_executor/gpu/asm_compiler.cc:77] Couldn't get ptxas version string: Internal: Couldn't invoke ptxas --version\n",
      "2022-10-15 13:27:29.473474: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-10-15 13:27:29.473527: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 131/131 - loss: 5.9989 - dice_loss: 0.9778 - disc_loss: 1.1099\n",
      "               loss_val: 5.0994 - dice_loss_val: 0.9702 - disc_loss_val: 0.2486 \n",
      "Validation loss decresaed from inf to 5.0994. Models' weights are now saved.\n",
      " \n",
      "Epoch 2/200\n",
      "Batch: 131/131 - loss: 5.7415 - dice_loss: 0.9559 - disc_loss: 0.9621\n",
      "               loss_val: 5.2313 - dice_loss_val: 0.9350 - disc_loss_val: 0.5565 \n",
      "Validation loss did not decrese from 5.0994.\n",
      " \n",
      "Epoch 3/200\n",
      "Batch: 131/131 - loss: 5.5313 - dice_loss: 0.9229 - disc_loss: 0.9169\n",
      "               loss_val: 5.0574 - dice_loss_val: 0.9037 - disc_loss_val: 0.5388 \n",
      "Validation loss decresaed from 5.0994 to 5.0574. Models' weights are now saved.\n",
      " \n",
      "Epoch 4/200\n",
      "Batch: 131/131 - loss: 5.2241 - dice_loss: 0.8736 - disc_loss: 0.8561\n",
      "               loss_val: 4.8403 - dice_loss_val: 0.8376 - disc_loss_val: 0.6522 \n",
      "Validation loss decresaed from 5.0574 to 4.8403. Models' weights are now saved.\n",
      " \n",
      "Epoch 5/200\n",
      "Batch: 131/131 - loss: 4.6532 - dice_loss: 0.7712 - disc_loss: 0.7971\n",
      "               loss_val: 4.1664 - dice_loss_val: 0.7148 - disc_loss_val: 0.5922 \n",
      "Validation loss decresaed from 4.8403 to 4.1664. Models' weights are now saved.\n",
      " \n",
      "Epoch 6/200\n",
      "Batch: 131/131 - loss: 3.9568 - dice_loss: 0.6406 - disc_loss: 0.7536\n",
      "               loss_val: 3.5173 - dice_loss_val: 0.5969 - disc_loss_val: 0.5325 \n",
      "Validation loss decresaed from 4.1664 to 3.5173. Models' weights are now saved.\n",
      " \n",
      "Epoch 7/200\n",
      "Batch: 131/131 - loss: 3.4168 - dice_loss: 0.5422 - disc_loss: 0.7057\n",
      "               loss_val: 3.1777 - dice_loss_val: 0.5147 - disc_loss_val: 0.6041 \n",
      "Validation loss decresaed from 3.5173 to 3.1777. Models' weights are now saved.\n",
      " \n",
      "Epoch 8/200\n",
      "Batch: 131/131 - loss: 3.0200 - dice_loss: 0.4741 - disc_loss: 0.6496\n",
      "               loss_val: 3.2006 - dice_loss_val: 0.4716 - disc_loss_val: 0.8424 \n",
      "Validation loss did not decrese from 3.1777.\n",
      " \n",
      "Epoch 9/200\n",
      "Batch: 131/131 - loss: 2.7534 - dice_loss: 0.4288 - disc_loss: 0.6094\n",
      "               loss_val: 3.0315 - dice_loss_val: 0.4406 - disc_loss_val: 0.8284 \n",
      "Validation loss decresaed from 3.1777 to 3.0315. Models' weights are now saved.\n",
      " \n",
      "Epoch 10/200\n",
      "Batch: 131/131 - loss: 2.5419 - dice_loss: 0.3931 - disc_loss: 0.5765\n",
      "               loss_val: 2.9522 - dice_loss_val: 0.4226 - disc_loss_val: 0.8391 \n",
      "Validation loss decresaed from 3.0315 to 2.9522. Models' weights are now saved.\n",
      " \n",
      "Epoch 11/200\n",
      "Batch: 131/131 - loss: 2.3892 - dice_loss: 0.3668 - disc_loss: 0.5554\n",
      "               loss_val: 2.8385 - dice_loss_val: 0.4084 - disc_loss_val: 0.7964 \n",
      "Validation loss decresaed from 2.9522 to 2.8385. Models' weights are now saved.\n",
      " \n",
      "Epoch 12/200\n",
      "Batch: 131/131 - loss: 2.2549 - dice_loss: 0.3442 - disc_loss: 0.5337\n",
      "               loss_val: 2.3701 - dice_loss_val: 0.3838 - disc_loss_val: 0.4510 \n",
      "Validation loss decresaed from 2.8385 to 2.3701. Models' weights are now saved.\n",
      " \n",
      "Epoch 13/200\n",
      "Batch: 131/131 - loss: 2.1299 - dice_loss: 0.3261 - disc_loss: 0.4992\n",
      "               loss_val: 2.4356 - dice_loss_val: 0.3861 - disc_loss_val: 0.5049 \n",
      "Validation loss did not decrese from 2.3701.\n",
      " \n",
      "Epoch 14/200\n",
      "Batch: 131/131 - loss: 2.0203 - dice_loss: 0.3127 - disc_loss: 0.4567\n",
      "               loss_val: 2.4552 - dice_loss_val: 0.3741 - disc_loss_val: 0.5848 \n",
      "Validation loss did not decrese from 2.3701.\n",
      " \n",
      "Epoch 15/200\n",
      "Batch: 131/131 - loss: 1.9642 - dice_loss: 0.3027 - disc_loss: 0.4505\n",
      "               loss_val: 2.2865 - dice_loss_val: 0.3624 - disc_loss_val: 0.4747 \n",
      "Validation loss decresaed from 2.3701 to 2.2865. Models' weights are now saved.\n",
      " \n",
      "Epoch 16/200\n",
      "Batch: 131/131 - loss: 1.8397 - dice_loss: 0.2915 - disc_loss: 0.3820\n",
      "               loss_val: 2.3746 - dice_loss_val: 0.3652 - disc_loss_val: 0.5488 \n",
      "Validation loss did not decrese from 2.2865.\n",
      " \n",
      "Epoch 17/200\n",
      "Batch: 131/131 - loss: 1.8074 - dice_loss: 0.2801 - disc_loss: 0.4069\n",
      "               loss_val: 2.4213 - dice_loss_val: 0.3623 - disc_loss_val: 0.6096 \n",
      "Validation loss did not decrese from 2.2865.\n",
      " \n",
      "Epoch 18/200\n",
      "Batch: 131/131 - loss: 1.7396 - dice_loss: 0.2708 - disc_loss: 0.3854\n",
      "               loss_val: 2.4135 - dice_loss_val: 0.3597 - disc_loss_val: 0.6149 \n",
      "Validation loss did not decrese from 2.2865.\n",
      " \n",
      "Epoch 19/200\n",
      "Batch: 131/131 - loss: 1.7148 - dice_loss: 0.2621 - disc_loss: 0.4045\n",
      "               loss_val: 2.3726 - dice_loss_val: 0.3641 - disc_loss_val: 0.5521 \n",
      "Validation loss did not decrese from 2.2865.\n",
      " \n",
      "Epoch 20/200\n",
      "Batch: 131/131 - loss: 1.6697 - dice_loss: 0.2558 - disc_loss: 0.3905\n",
      "               loss_val: 2.2688 - dice_loss_val: 0.3616 - disc_loss_val: 0.4605 \n",
      "Validation loss decresaed from 2.2865 to 2.2688. Models' weights are now saved.\n",
      " \n",
      "Epoch 21/200\n",
      "Batch: 131/131 - loss: 1.6286 - dice_loss: 0.2495 - disc_loss: 0.3810\n",
      "               loss_val: 2.2977 - dice_loss_val: 0.3650 - disc_loss_val: 0.4729 \n",
      "Validation loss did not decrese from 2.2688.\n",
      " \n",
      "Epoch 22/200\n",
      "Batch: 131/131 - loss: 1.6012 - dice_loss: 0.2448 - disc_loss: 0.3772\n",
      "               loss_val: 2.2033 - dice_loss_val: 0.3629 - disc_loss_val: 0.3888 \n",
      "Validation loss decresaed from 2.2688 to 2.2033. Models' weights are now saved.\n",
      " \n",
      "Epoch 23/200\n",
      "Batch: 131/131 - loss: 1.5604 - dice_loss: 0.2400 - disc_loss: 0.3605\n",
      "               loss_val: 2.2514 - dice_loss_val: 0.3626 - disc_loss_val: 0.4386 \n",
      "Validation loss did not decrese from 2.2033.\n",
      " \n",
      "Epoch 24/200\n",
      "Batch: 131/131 - loss: 1.5456 - dice_loss: 0.2348 - disc_loss: 0.3718\n",
      "               loss_val: 2.1979 - dice_loss_val: 0.3647 - disc_loss_val: 0.3744 \n",
      "Validation loss decresaed from 2.2033 to 2.1979. Models' weights are now saved.\n",
      " \n",
      "Epoch 25/200\n",
      "Batch: 131/131 - loss: 1.5224 - dice_loss: 0.2301 - disc_loss: 0.3721\n",
      "               loss_val: 2.2408 - dice_loss_val: 0.3655 - disc_loss_val: 0.4135 \n",
      "Validation loss did not decrese from 2.1979.\n",
      " \n",
      "Epoch 26/200\n",
      "Batch: 131/131 - loss: 1.5141 - dice_loss: 0.2291 - disc_loss: 0.3685\n",
      "               loss_val: 2.1963 - dice_loss_val: 0.3618 - disc_loss_val: 0.3871 \n",
      "Validation loss decresaed from 2.1979 to 2.1963. Models' weights are now saved.\n",
      " \n",
      "Epoch 27/200\n",
      "Batch: 131/131 - loss: 1.4942 - dice_loss: 0.2249 - disc_loss: 0.3698\n",
      "               loss_val: 2.2926 - dice_loss_val: 0.3570 - disc_loss_val: 0.5075 \n",
      "Validation loss did not decrese from 2.1963.\n",
      " \n",
      "Epoch 28/200\n",
      "Batch: 131/131 - loss: 1.4600 - dice_loss: 0.2199 - disc_loss: 0.3607\n",
      "               loss_val: 2.2291 - dice_loss_val: 0.3550 - disc_loss_val: 0.4542 \n",
      "Validation loss did not decrese from 2.1963.\n",
      " \n",
      "Epoch 29/200\n",
      "Batch: 131/131 - loss: 1.4318 - dice_loss: 0.2160 - disc_loss: 0.3516\n",
      "               loss_val: 2.2586 - dice_loss_val: 0.3598 - disc_loss_val: 0.4596 \n",
      "Validation loss did not decrese from 2.1963.\n",
      " \n",
      "Epoch 30/200\n",
      "Batch: 131/131 - loss: 1.4069 - dice_loss: 0.2140 - disc_loss: 0.3367\n",
      "               loss_val: 2.1718 - dice_loss_val: 0.3561 - disc_loss_val: 0.3911 \n",
      "Validation loss decresaed from 2.1963 to 2.1718. Models' weights are now saved.\n",
      " \n",
      "Epoch 31/200\n",
      "Batch: 131/131 - loss: 1.3988 - dice_loss: 0.2149 - disc_loss: 0.3240\n",
      "               loss_val: 2.1823 - dice_loss_val: 0.3535 - disc_loss_val: 0.4150 \n",
      "Validation loss did not decrese from 2.1718.\n",
      " \n",
      "Epoch 32/200\n",
      "Batch: 131/131 - loss: 1.3797 - dice_loss: 0.2082 - disc_loss: 0.3387\n",
      "               loss_val: 2.2010 - dice_loss_val: 0.3608 - disc_loss_val: 0.3972 \n",
      "Validation loss did not decrese from 2.1718.\n",
      " \n",
      "Epoch 33/200\n",
      "Batch: 131/131 - loss: 1.3723 - dice_loss: 0.2044 - disc_loss: 0.3503\n",
      "               loss_val: 2.2097 - dice_loss_val: 0.3593 - disc_loss_val: 0.4130 \n",
      "Validation loss did not decrese from 2.1718.\n",
      " \n",
      "Epoch 34/200\n",
      "Batch: 131/131 - loss: 1.3854 - dice_loss: 0.2069 - disc_loss: 0.3510\n",
      "               loss_val: 2.4114 - dice_loss_val: 0.3541 - disc_loss_val: 0.6411 \n",
      "Validation loss did not decrese from 2.1718.\n",
      " \n",
      "Epoch 35/200\n",
      "Batch: 131/131 - loss: 1.3638 - dice_loss: 0.2047 - disc_loss: 0.3402\n",
      "               loss_val: 2.3083 - dice_loss_val: 0.3532 - disc_loss_val: 0.5425 \n",
      "Validation loss did not decrese from 2.1718.\n",
      " \n",
      "Epoch 36/200\n",
      "Batch: 131/131 - loss: 1.3631 - dice_loss: 0.2022 - disc_loss: 0.3521\n",
      "               loss_val: 2.2446 - dice_loss_val: 0.3517 - disc_loss_val: 0.4860 \n",
      "Validation loss did not decrese from 2.1718.\n",
      " \n",
      "Epoch 37/200\n",
      "Batch: 131/131 - loss: 1.3390 - dice_loss: 0.1997 - disc_loss: 0.3404\n",
      "               loss_val: 2.2514 - dice_loss_val: 0.3659 - disc_loss_val: 0.4218 \n",
      "Validation loss did not decrese from 2.1718.\n",
      " \n",
      "Epoch 38/200\n",
      "Batch: 131/131 - loss: 1.3344 - dice_loss: 0.1984 - disc_loss: 0.3424\n",
      "               loss_val: 2.5211 - dice_loss_val: 0.3575 - disc_loss_val: 0.7339 \n",
      "Validation loss did not decrese from 2.1718.\n",
      " \n",
      "Epoch 39/200\n",
      "Batch: 131/131 - loss: 1.3358 - dice_loss: 0.1977 - disc_loss: 0.3474\n",
      "               loss_val: 2.2553 - dice_loss_val: 0.3600 - disc_loss_val: 0.4552 \n",
      "Validation loss did not decrese from 2.1718.\n",
      " \n",
      "Epoch 40/200\n",
      "Batch: 131/131 - loss: 1.3343 - dice_loss: 0.1965 - disc_loss: 0.3515\n",
      "               loss_val: 2.2306 - dice_loss_val: 0.3604 - disc_loss_val: 0.4288 \n",
      "Validation loss did not decrese from 2.1718.\n",
      " \n",
      "Epoch 41/200\n",
      "Batch: 131/131 - loss: 1.3093 - dice_loss: 0.1955 - disc_loss: 0.3320\n",
      "               loss_val: 2.3852 - dice_loss_val: 0.3565 - disc_loss_val: 0.6025 \n",
      "Validation loss did not decrese from 2.1718.\n",
      " \n",
      "Epoch 42/200\n",
      "Batch: 131/131 - loss: 1.3006 - dice_loss: 0.1915 - disc_loss: 0.3433\n",
      "               loss_val: 2.3325 - dice_loss_val: 0.3579 - disc_loss_val: 0.5431 \n",
      "Validation loss did not decrese from 2.1718.\n",
      " \n",
      "Epoch 43/200\n",
      "Batch: 131/131 - loss: 1.2836 - dice_loss: 0.1881 - disc_loss: 0.3428\n",
      "               loss_val: 2.5059 - dice_loss_val: 0.3586 - disc_loss_val: 0.7131 \n",
      "Validation loss did not decrese from 2.1718.\n",
      " \n",
      "Epoch 44/200\n",
      "Batch: 131/131 - loss: 1.3016 - dice_loss: 0.1890 - disc_loss: 0.3565\n",
      "               loss_val: 2.4340 - dice_loss_val: 0.3574 - disc_loss_val: 0.6470 \n",
      "Validation loss did not decrese from 2.1718.\n",
      " \n",
      "Epoch 45/200\n",
      "Batch: 131/131 - loss: 1.2855 - dice_loss: 0.1865 - disc_loss: 0.3530\n",
      "               loss_val: 2.5205 - dice_loss_val: 0.3739 - disc_loss_val: 0.6512 \n",
      "Validation loss did not decrese from 2.1718.\n",
      " \n",
      "Epoch 46/200\n",
      "Batch: 131/131 - loss: 1.2647 - dice_loss: 0.1832 - disc_loss: 0.3487\n",
      "               loss_val: 2.4747 - dice_loss_val: 0.3686 - disc_loss_val: 0.6316 \n",
      "Validation loss did not decrese from 2.1718.\n",
      " \n",
      "Epoch 47/200\n",
      "Batch: 131/131 - loss: 1.2656 - dice_loss: 0.1831 - disc_loss: 0.3500\n",
      "               loss_val: 2.5522 - dice_loss_val: 0.3583 - disc_loss_val: 0.7606 \n",
      "Validation loss did not decrese from 2.1718.\n",
      " \n",
      "Epoch 48/200\n",
      "Batch: 131/131 - loss: 1.2521 - dice_loss: 0.1812 - disc_loss: 0.3459\n",
      "               loss_val: 2.5202 - dice_loss_val: 0.3637 - disc_loss_val: 0.7016 \n",
      "Validation loss did not decrese from 2.1718.\n",
      " \n",
      "Epoch 49/200\n",
      "Batch: 131/131 - loss: 1.2679 - dice_loss: 0.1824 - disc_loss: 0.3558\n",
      "               loss_val: 2.5521 - dice_loss_val: 0.3612 - disc_loss_val: 0.7460 \n",
      "Validation loss did not decrese from 2.1718.\n",
      " \n",
      "Epoch 50/200\n",
      "Batch: 131/131 - loss: 1.2422 - dice_loss: 0.1787 - disc_loss: 0.3486\n",
      "               loss_val: 2.2855 - dice_loss_val: 0.3739 - disc_loss_val: 0.4158 \n",
      "Validation loss did not decrese from 2.1718.\n",
      " \n",
      "Epoch 51/200\n",
      "Batch: 131/131 - loss: 1.2293 - dice_loss: 0.1772 - disc_loss: 0.3434\n",
      "               loss_val: 2.5168 - dice_loss_val: 0.3594 - disc_loss_val: 0.7200 \n",
      "Validation loss did not decrese from 2.1718.\n"
     ]
    }
   ],
   "source": [
    "alpha=5\n",
    "n_epochs = 200\n",
    "batch_size = 4\n",
    "\n",
    "traingen = DataGenerator('Train', batch_size=batch_size, img_size=(64,64,64,3), mask_size=(64,64,64,3))\n",
    "\n",
    "valgen = DataGenerator('Validation', batch_size=batch_size, img_size=(64,64,64,3), mask_size=(64,64,64,3))\n",
    "\n",
    "# train the vox2vox model\n",
    "h = fit(traingen, valgen, alpha, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "iOhnNabGSjOU"
   },
   "outputs": [],
   "source": [
    "def model_plots(history, plot_dir):\n",
    "  #plot the training and validation IoU and loss at each epoch\n",
    "    loss = np.array(history['train'][:])[:,1]\n",
    "    val_loss = np.array(history['valid'][:])[:,1]\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "    plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(plot_dir + '/loss_plot.png')\n",
    "    plt.clf()\n",
    "\n",
    "  #acc = history.history['dice_coefficient']\n",
    "  #val_acc = history.history['val_dice_coefficient']\n",
    "\n",
    "  #plt.plot(epochs, acc, 'y', label='Training Dice')\n",
    "  #plt.plot(epochs, val_acc, 'r', label='Validation Dice')\n",
    "  #plt.title('Training and validation Dice')\n",
    "  #plt.xlabel('Epochs')\n",
    "  #plt.ylabel('Dice')\n",
    "  #plt.legend()\n",
    "  #plt.savefig(plot_dir + '/dice_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "wI8ZSd1wSjOV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_dir = './Dataset_3/Model_Plots_Pix2Pix'\n",
    "if os.path.exists(plot_dir)==False:\n",
    "    os.mkdir(plot_dir)\n",
    "\n",
    "model_plots(h, plot_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0l9Y_XNSjOW",
    "tags": []
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ryDH6VEVSjOW"
   },
   "outputs": [],
   "source": [
    "def padding(image,size):\n",
    "\n",
    "    img_reshape = np.moveaxis(image, -1, 0)\n",
    "    \n",
    "    m = ReflectionPad3d((0,size[2]-image.shape[2],0,size[1]-image.shape[1],0,size[0]-image.shape[0]))\n",
    "    input = torch.tensor(img_reshape, dtype=torch.float)\n",
    "    output = m(input)\n",
    "    pad_img = output.numpy()\n",
    "    pad_img = np.moveaxis(pad_img, 0, -1)\n",
    "    \n",
    "    return pad_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "6v7kIgY8SjOX"
   },
   "outputs": [],
   "source": [
    "def thresholding(patch_prediction):\n",
    "    patch_ind_nuclei = np.argwhere(patch_prediction[:,:,:,0] > 0.5)\n",
    "    patch_ind_golgi = np.argwhere(patch_prediction[:,:,:,1] > 0.5)\n",
    "    patch_ind_int = np.argwhere(patch_prediction[:,:,:,2] > 0.5)\n",
    "\n",
    "    patch_prediction_thr = np.zeros((patch_prediction.shape[0],patch_prediction.shape[1],patch_prediction.shape[2],3))\n",
    "\n",
    "    for i in range(patch_ind_nuclei.shape[0]):\n",
    "        patch_prediction_thr[patch_ind_nuclei[i,0],patch_ind_nuclei[i,1],patch_ind_nuclei[i,2],0]=1 \n",
    "\n",
    "    for j in range(patch_ind_golgi.shape[0]):\n",
    "        patch_prediction_thr[patch_ind_golgi[j,0],patch_ind_golgi[j,1],patch_ind_golgi[j,2],1]=1\n",
    "    \n",
    "    for k in range(patch_ind_int.shape[0]):\n",
    "        patch_prediction_thr[patch_ind_int[k,0],patch_ind_int[k,1],patch_ind_int[k,2],2]=1\n",
    "\n",
    "    return patch_prediction_thr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "cWKNfGvqSjOX"
   },
   "outputs": [],
   "source": [
    "def pred_mask(image, mask_shape):\n",
    "  patch_size = 64\n",
    "  step = 48\n",
    "\n",
    "  pred_mask = np.zeros(((image.shape[0] // step)*step + patch_size,(image.shape[1] // step)*step + patch_size,64,3))\n",
    "  _image = padding(image, ((image.shape[0] // step)*step + patch_size,(image.shape[1] // step)*step + patch_size,64,3))\n",
    "\n",
    "  i = 0\n",
    "  while i + patch_size <= _image.shape[0]:\n",
    "    j = 0\n",
    "    while j + patch_size <= _image.shape[1]:\n",
    "\n",
    "        tst_patch = _image[i:i+patch_size, j:j+patch_size, :, :]\n",
    "        tst_patch = np.array([tst_patch / 255.])\n",
    "        preds_tst = G(tst_patch, training=False)\n",
    "        pred_patch = preds_tst[0,:,:,:,:]\n",
    "        pred_patch_thr = thresholding(pred_patch)\n",
    "\n",
    "        pred_mask[i:i+patch_size, j:j+patch_size,:,0] = np.array(np.logical_or(pred_mask[i:i+patch_size, j:j+patch_size, :,0], pred_patch_thr[:,:,:,0], dtype = 'float32'))\n",
    "        pred_mask[i:i+patch_size, j:j+patch_size,:,1] = np.array(np.logical_or(pred_mask[i:i+patch_size, j:j+patch_size, :,1], pred_patch_thr[:,:,:,1], dtype = 'float32'))\n",
    "        pred_mask[i:i+patch_size, j:j+patch_size,:,2] = np.array(np.logical_or(pred_mask[i:i+patch_size, j:j+patch_size, :,2], pred_patch_thr[:,:,:,2], dtype = 'float32'))\n",
    "\n",
    "        j += step\n",
    "        \n",
    "    i += step\n",
    "    \n",
    "  _pred_mask = np.zeros(mask_shape)\n",
    "  _pred_mask = pred_mask[0:mask_shape[0],0:mask_shape[1],0:mask_shape[2],:]\n",
    "\n",
    "  return _pred_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "JJVNFuHESjOY"
   },
   "outputs": [],
   "source": [
    "def dice_coefficient(y_true, y_pred):\n",
    "    smoothing_factor = 1\n",
    "    flat_y_true = K.flatten(y_true)\n",
    "    flat_y_pred = K.flatten(y_pred)\n",
    "    return (2. * K.sum(flat_y_true * flat_y_pred) + smoothing_factor) / (K.sum(flat_y_true) + K.sum(flat_y_pred) + smoothing_factor)\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    flat_y_true = K.flatten(y_true)\n",
    "    flat_y_pred = K.flatten(y_pred)\n",
    "    return(K.sum(flat_y_true * flat_y_pred) / K.sum(flat_y_pred) )\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    flat_y_true = K.flatten(y_true)\n",
    "    flat_y_pred = K.flatten(y_pred)\n",
    "    return(K.sum(flat_y_true * flat_y_pred) / K.sum(flat_y_true) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "g6xzOEs4SjOY"
   },
   "outputs": [],
   "source": [
    "def metrics(mask, _pred_mask):\n",
    "  _mask = mask/255.\n",
    "\n",
    "  dice_coef_int = dice_coefficient(_mask[:,:,:,2],_pred_mask[:,:,:,2])\n",
    "  dice_coef_nuclei = dice_coefficient(_mask[:,:,:,1],_pred_mask[:,:,:,1])\n",
    "  dice_coef_golgi = dice_coefficient(_mask[:,:,:,0],_pred_mask[:,:,:,0])\n",
    "  prec_int = precision(_mask[:,:,:,2],_pred_mask[:,:,:,2])\n",
    "  prec_nuclei = precision(_mask[:,:,:,1],_pred_mask[:,:,:,1])\n",
    "  prec_golgi = precision(_mask[:,:,:,0],_pred_mask[:,:,:,0])\n",
    "  recall_int = recall(_mask[:,:,:,2],_pred_mask[:,:,:,2])\n",
    "  recall_nuclei = recall(_mask[:,:,:,1],_pred_mask[:,:,:,1])\n",
    "  recall_golgi = recall(_mask[:,:,:,0],_pred_mask[:,:,:,0])\n",
    "\n",
    "  return [round(dice_coef_nuclei.numpy(),4), round(dice_coef_golgi.numpy(),4),round(dice_coef_int.numpy(),4), \n",
    "          round(prec_nuclei.numpy(),4), round(prec_golgi.numpy(),4),round(prec_int.numpy(),4), round(recall_nuclei.numpy(),4),\n",
    "          round(recall_golgi.numpy(),4), round(recall_int.numpy(),4)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "qopqvtRzSjOZ"
   },
   "outputs": [],
   "source": [
    "def write_to_excel(wb,sheet_name,metrics, result_dir):\n",
    "\n",
    "  # add_sheet is used to create sheet.\n",
    "  sheet1 = wb.add_sheet(sheet_name)\n",
    "\n",
    "  sheet1.write(0, 0, 'Dice Coeffient Nuclei')\n",
    "  sheet1.write(0, 1, 'Dice Coeffient Golgi')\n",
    "  sheet1.write(0, 2, 'Dice Coeffient Intersection')\n",
    "  sheet1.write(0, 3, 'Precision Nuclei')\n",
    "  sheet1.write(0, 4, 'Precision Golgi')\n",
    "  sheet1.write(0, 5, 'Precision Intersection')\n",
    "  sheet1.write(0, 6, 'Recall Nuclei')\n",
    "  sheet1.write(0, 7, 'Recall Golgi')\n",
    "  sheet1.write(0, 8, 'Recall Intersection')\n",
    "  sheet1.write(1, 0, metrics[0])\n",
    "  sheet1.write(1, 1, metrics[1])\n",
    "  sheet1.write(1, 2, metrics[2])\n",
    "  sheet1.write(1, 3, metrics[3])\n",
    "  sheet1.write(1, 4, metrics[4])\n",
    "  sheet1.write(1, 5, metrics[5])\n",
    "  sheet1.write(1, 6, metrics[6])\n",
    "  sheet1.write(1, 7, metrics[7])\n",
    "  sheet1.write(1, 8, metrics[8])\n",
    "\n",
    "  wb.save(result_dir + '/results_metrics.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "LsKxziyzSjOZ"
   },
   "outputs": [],
   "source": [
    "def test_model(base_dir):\n",
    "\n",
    "  fnames = os.listdir(base_dir + 'Images/')\n",
    "\n",
    "  # Workbook is created\n",
    "  wb = Workbook()\n",
    "\n",
    "  result_dir = './Dataset_3/Results_Pix2Pix'\n",
    "  if os.path.exists(result_dir)==False:\n",
    "    os.mkdir(result_dir)\n",
    "\n",
    "  for i in range(len(fnames)):\n",
    "\n",
    "    image = imread(base_dir + 'Images/' + fnames[i])\n",
    "    mask = imread(base_dir + 'Masks/' + fnames[i])\n",
    "\n",
    "    predicted_mask = pred_mask(image, image.shape)\n",
    "\n",
    "    _predicted_mask = predicted_mask*255.0\n",
    "    _predicted_mask = _predicted_mask.astype('uint8')\n",
    "\n",
    "    imwrite(result_dir + '/pred_mask_' + fnames[i] , _predicted_mask, photometric='rgb')\n",
    "\n",
    "    _metrics = metrics(mask, predicted_mask)\n",
    "\n",
    "    write_to_excel(wb,'Sheet_' + fnames[i].split('.')[0], _metrics, result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44702,
     "status": "ok",
     "timestamp": 1658139209619,
     "user": {
      "displayName": "Alice Rosa",
      "userId": "08910053332263565279"
     },
     "user_tz": -60
    },
    "id": "WURo2qAsSjOa",
    "outputId": "821ab121-2e45-42f9-de4b-65c34d21dbbd"
   },
   "outputs": [],
   "source": [
    "#Load the pretrained model for testing and predictions. \n",
    "\n",
    "img_shape=(64,64,64,3)\n",
    "G = Generator(img_shape)\n",
    "G.load_weights('./Models_Pix2Pix/Generator_3channel.h5')\n",
    "\n",
    "base_dir = './Dataset_3/Patches_64/Test/'\n",
    "test_model(base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TbaVGWKy6Eu4",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Gerar mascaras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 241,
     "status": "ok",
     "timestamp": 1658142472974,
     "user": {
      "displayName": "Alice Rosa",
      "userId": "08910053332263565279"
     },
     "user_tz": -60
    },
    "id": "KpjbK92v1d_j"
   },
   "outputs": [],
   "source": [
    "def generate_synthetic_masks(train_dir_img, train_dir_mask, n_patches):\n",
    "\n",
    "  size_img = (128,128,64,3)\n",
    "  size_mask = (128,128,64,2)\n",
    "\n",
    "  for p in range(n_patches):\n",
    "      img = np.zeros(size_img)\n",
    "      img[:,:,:,0] = np.random.uniform(low=10, high=20, size=(128,128,64))\n",
    "      img[:,:,:,1] = np.random.uniform(low=5, high=15, size=(128,128,64))\n",
    "      img = img/255.\n",
    "      imwrite(train_dir_img + '/img_patch_' + str(5) + str(p) + '.tif', img)\n",
    "\n",
    "  for k in range(n_patches):\n",
    "      patch = np.zeros(size_mask)\n",
    "      patch = patch/255.\n",
    "      imwrite(train_dir_mask + '/img_patch_' + str(5) + str(k) + '.tif', patch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "executionInfo": {
     "elapsed": 14319,
     "status": "ok",
     "timestamp": 1658142488366,
     "user": {
      "displayName": "Alice Rosa",
      "userId": "08910053332263565279"
     },
     "user_tz": -60
    },
    "id": "XswuE6Z3-0f5"
   },
   "outputs": [],
   "source": [
    "generate_synthetic_masks('./Dataset/Patches_128/Train/Images', './Dataset/Patches_128/Train/Masks', 12)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "5Ib54cjC0nmf"
   ],
   "name": "3D-Pix2Pix-V3.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
